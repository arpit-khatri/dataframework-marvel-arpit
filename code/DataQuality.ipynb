{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78226c88-0329-47cb-a216-07b1127ec585",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 2.161s\n",
      "\n",
      "OK\n",
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 1.591s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'empty'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 139\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Find the corresponding row in test_results_df\u001b[39;00m\n\u001b[1;32m    137\u001b[0m test_result_row \u001b[38;5;241m=\u001b[39m test_results_df[test_results_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTable Name\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m table_name]\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mtest_result_row\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m:\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# Update the corresponding row in existing_load_status_df\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     existing_load_status_df\u001b[38;5;241m.\u001b[39mloc[index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStatus\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m test_result_row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStatus\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    142\u001b[0m     existing_load_status_df\u001b[38;5;241m.\u001b[39mloc[index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m test_result_row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Type\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:2977\u001b[0m, in \u001b[0;36mDataFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2944\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001b[39;00m\n\u001b[1;32m   2945\u001b[0m \n\u001b[1;32m   2946\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2974\u001b[0m \u001b[38;5;124;03m+---+\u001b[39;00m\n\u001b[1;32m   2975\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2976\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m-> 2977\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   2978\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name)\n\u001b[1;32m   2979\u001b[0m     )\n\u001b[1;32m   2980\u001b[0m jc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mapply(name)\n\u001b[1;32m   2981\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Column(jc)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'empty'"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "import yaml\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "class TestCharStats(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        # Loading configuration from config.yaml\n",
    "        with open(\"config.yaml\", \"r\") as config_file:\n",
    "            config = yaml.safe_load(config_file)\n",
    "            \n",
    "        # Create a Spark session with a custom configuration\n",
    "        self.spark = SparkSession.builder \\\n",
    "            .appName(\"TestCharStats\") \\\n",
    "            .getOrCreate()\n",
    "        # Set the log level to ERROR or FATAL\n",
    "        self.spark.sparkContext.setLogLevel(\"ERROR\")  # You can change \"ERROR\" to \"FATAL\" if needed\n",
    "        \n",
    "        # Assign configuration variables\n",
    "        self.table_name = config[\"table_name\"]\n",
    "        self.primary_key_columns = config[\"primary_key_columns\"]\n",
    "        self.threshold_percentage = config[\"threshold_percentage\"]\n",
    "        self.output_path = config[\"output_path\"]\n",
    "        self.absolute_table_name =f\"{self.output_path}/{self.table_name}\"\n",
    "\n",
    "    def tearDown(self):\n",
    "        # Stop the Spark session\n",
    "        self.spark.stop()\n",
    "\n",
    "    def log_test_result(self, test_name, status, reason=\"\"):\n",
    "        timestamp = datetime.datetime.now()\n",
    "        # Creating a dictionary for the test result\n",
    "        result = {\n",
    "            \"Table Name\": self.table_name,\n",
    "            \"Test Name\": test_name,\n",
    "            \"Status\": status,\n",
    "            \"Reason\": reason,\n",
    "            \"Timestamp\": timestamp\n",
    "        }\n",
    "        # Append the result to the test_results list\n",
    "        test_results.append(result)\n",
    "\n",
    "    def test_primary_key(self):\n",
    "        # Loading the current data\n",
    "        df = self.spark.read.parquet(self.absolute_table_name)  \n",
    "\n",
    "        # Checking primary key columns to make sure no duplicates\n",
    "        num_rows = df.count()\n",
    "        num_distinct_rows = df.select(*self.primary_key_columns).distinct().count()\n",
    "        #self.assertEqual(num_rows, num_distinct_rows)\n",
    "        if num_rows == num_distinct_rows:\n",
    "            self.log_test_result(\"test_primary_key\", \"PASS\")\n",
    "        else:\n",
    "            count_duplicate_key = num_rows - num_distinct_rows\n",
    "            self.log_test_result(\"test_primary_key\", \"FAIL\", f\"total of {count_duplicate_key} Duplicate primary key values found.\")\n",
    "\n",
    "    def test_count_increase(self):\n",
    "        # Loading the today's and previous day data for the comparision of countyour DataFrames for previous and current loads\n",
    "        df_previous = self.spark.read.parquet(self.absolute_table_name)\n",
    "        df_current = self.spark.read.parquet(self.absolute_table_name)\n",
    "\n",
    "        # Calculating the count of increased percentage\n",
    "        count_previous = df_previous.count()\n",
    "        count_current = df_current.count()\n",
    "        increase_percentage = (count_current - count_previous) / count_previous * 100\n",
    "\n",
    "        if increase_percentage >= self.threshold_percentage:\n",
    "            self.log_test_result(\"test_count_increase\", \"FAIL\", f\"Count_increase percentage is more than threshold\")\n",
    "        else:\n",
    "            self.log_test_result(\"test_count_increase\", \"PASS\", \"Count_increase percentage is below threshold\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Creating an empty list to store test results\n",
    "    test_results = []\n",
    "\n",
    "    # Creating a test suite\n",
    "    test_suite = unittest.TestLoader().loadTestsFromTestCase(TestCharStats)\n",
    "\n",
    "    # Running the tests\n",
    "    test_runner = unittest.TextTestRunner()\n",
    "\n",
    "    # Run each test and log the results\n",
    "    for test_case in test_suite:\n",
    "        test_result = test_runner.run(test_case)\n",
    "\n",
    "    # Converting the test results to write into a file\n",
    "    results_df = pd.DataFrame(test_results)\n",
    "    \n",
    "\n",
    "    # Write the results to a CSV file\n",
    "    results_df.to_csv(\"test_results.csv\", index=False)\n",
    "    results_df.head()\n",
    "    # Create a summary DataFrame to store load status\n",
    "    load_status_df = results_df.groupby([\"Table Name\"])[\"Status\"].max().reset_index()\n",
    "    load_status_df.loc[:, \"Test Type\"] = \"Data Quality Checks\"\n",
    "    \n",
    "    # Check if any row in the test results DataFrame has \"Failed\" in the \"Status\" column\n",
    "    if \"FAILED\" in load_status_df[\"Status\"].str.upper().values:\n",
    "        # Set the status in the summary DataFrame to \"Failed\"\n",
    "        load_status_df.loc[:, \"Status\"] = \"FAIL\"\n",
    "    else:\n",
    "        load_status_df.loc[:, \"Status\"] = \"PASS\"\n",
    "    \n",
    "    # Add a Date column with the current date\n",
    "    current_date = pd.to_datetime('today').date()\n",
    "    load_status_df[\"Date\"] = current_date\n",
    "    \n",
    "    # Display the summary load status\n",
    "    #load_status_df.to_csv(\"load_status.csv\", index=False)\n",
    "\n",
    "    # Define the file path for the load status CSV\n",
    "    csv_file_path = \"load_status.csv\"\n",
    "    \n",
    "    # Check if the file already exists\n",
    "    if os.path.isfile(csv_file_path):\n",
    "        # Read the existing CSV file\n",
    "        existing_load_status_df = pd.read_csv(csv_file_path)\n",
    "        \n",
    "        # Check if the date matches the current date\n",
    "        #current_date = pd.to_datetime('today').date()\n",
    "        #matching_date_rows = existing_load_status_df[existing_load_status_df[\"Date\"] == current_date]\n",
    "\n",
    "        # Assuming 'Date' column is currently of string type\n",
    "        existing_load_status_df['Date'] = pd.to_datetime(existing_load_status_df['Date'])\n",
    "        \n",
    "        current_date = pd.to_datetime('today').date()\n",
    "        matching_date_rows = existing_load_status_df[existing_load_status_df['Date'].dt.date == current_date]\n",
    "    \n",
    "        if matching_date_rows.shape[0] > 0:\n",
    "            # Iterate over each row in matching_date_rows and update existing_load_status_df\n",
    "            for index, row in matching_date_rows.iterrows():\n",
    "                table_name = row[\"Table Name\"]\n",
    "    \n",
    "                # Find the corresponding row in test_results_df\n",
    "                test_result_row = test_results_df[test_results_df[\"Table Name\"] == table_name]\n",
    "    \n",
    "                if not test_result_row.empty:\n",
    "                    # Update the corresponding row in existing_load_status_df\n",
    "                    existing_load_status_df.loc[index, \"Status\"] = test_result_row[\"Status\"].values[0]\n",
    "                    existing_load_status_df.loc[index, \"Test Type\"] = test_result_row[\"Test Type\"].values[0]\n",
    "                else:\n",
    "                    # If no matching row is found in test_results_df, remove the row from existing_load_status_df\n",
    "                    existing_load_status_df = existing_load_status_df.drop(index)\n",
    "        else:\n",
    "            # Append a new entry for a different date\n",
    "            new_row = {\"Table Name\": table_name, \"Status\": status, \"Test Type\": test_type, \"Date\": current_date}\n",
    "            existing_load_status_df = existing_load_status_df.append(new_row, ignore_index=True)\n",
    "else:\n",
    "    # The file doesn't exist, create it with the current load status\n",
    "    existing_load_status_df = test_results_df\n",
    "\n",
    "# Write the updated or new load status to the CSV file\n",
    "existing_load_status_df.to_csv(csv_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c90881d7-f054-4ebd-ae3f-5cbc1c5928cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 1.352s\n",
      "\n",
      "OK\n",
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 1.633s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------------+-------------------+----------+\n",
      "|        Table Name|          Status|          Test Type|      Date|\n",
      "+------------------+----------------+-------------------+----------+\n",
      "|char_stats_day_dly|test_primary_key|Data Quality Checks|2023-09-16|\n",
      "+------------------+----------------+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "import yaml\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, upper\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "class TestCharStats(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        # Loading configuration from config.yaml\n",
    "        with open(\"config.yaml\", \"r\") as config_file:\n",
    "            config = yaml.safe_load(config_file)\n",
    "            \n",
    "        # Create a Spark session with a custom configuration\n",
    "        self.spark = SparkSession.builder \\\n",
    "            .appName(\"TestCharStats\") \\\n",
    "            .getOrCreate()\n",
    "        # Set the log level to ERROR or FATAL\n",
    "        self.spark.sparkContext.setLogLevel(\"ERROR\")  # You can change \"ERROR\" to \"FATAL\" if needed\n",
    "        \n",
    "        # Assign configuration variables\n",
    "        self.table_name = config[\"table_name\"]\n",
    "        self.primary_key_columns = config[\"primary_key_columns\"]\n",
    "        self.threshold_percentage = config[\"threshold_percentage\"]\n",
    "        self.output_path = config[\"output_path\"]\n",
    "        self.absolute_table_name =f\"{self.output_path}/{self.table_name}\"\n",
    "\n",
    "    def tearDown(self):\n",
    "        # Stop the Spark session\n",
    "        self.spark.stop()\n",
    "\n",
    "    def log_test_result(self, test_name, status, reason=\"\"):\n",
    "        timestamp = datetime.datetime.now()\n",
    "        # Creating a dictionary for the test result\n",
    "        result = {\n",
    "            \"Table Name\": self.table_name,\n",
    "            \"Test Name\": test_name,\n",
    "            \"Status\": status,\n",
    "            \"Reason\": reason,\n",
    "            \"Timestamp\": timestamp\n",
    "        }\n",
    "        # Append the result to the test_results list\n",
    "        test_results.append(result)\n",
    "\n",
    "    def test_primary_key(self):\n",
    "        # Loading the current data\n",
    "        df = self.spark.read.parquet(self.absolute_table_name)  \n",
    "\n",
    "        # Checking primary key columns to make sure no duplicates\n",
    "        num_rows = df.count()\n",
    "        num_distinct_rows = df.select(*self.primary_key_columns).distinct().count()\n",
    "        if num_rows == num_distinct_rows:\n",
    "            self.log_test_result(\"test_primary_key\", \"PASS\")\n",
    "        else:\n",
    "            count_duplicate_key = num_rows - num_distinct_rows\n",
    "            self.log_test_result(\"test_primary_key\", \"FAIL\", f\"total of {count_duplicate_key} Duplicate primary key values found.\")\n",
    "\n",
    "    def test_count_increase(self):\n",
    "        # Loading the today's and previous day data for the comparision of countyour DataFrames for previous and current loads\n",
    "        df_previous = self.spark.read.parquet(self.absolute_table_name)\n",
    "        df_current = self.spark.read.parquet(self.absolute_table_name)\n",
    "\n",
    "        # Calculating the count of increased percentage\n",
    "        count_previous = df_previous.count()\n",
    "        count_current = df_current.count()\n",
    "        increase_percentage = (count_current - count_previous) / count_previous * 100\n",
    "\n",
    "        if increase_percentage >= self.threshold_percentage:\n",
    "            self.log_test_result(\"test_count_increase\", \"FAIL\", f\"Count_increase percentage is more than threshold\")\n",
    "        else:\n",
    "            self.log_test_result(\"test_count_increase\", \"PASS\", \"Count_increase percentage is below threshold\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Creating an empty list to store test results\n",
    "    test_results = []\n",
    "\n",
    "    # Creating a test suite\n",
    "    test_suite = unittest.TestLoader().loadTestsFromTestCase(TestCharStats)\n",
    "\n",
    "    # Running the tests\n",
    "    test_runner = unittest.TextTestRunner()\n",
    "\n",
    "    # Run each test and log the results\n",
    "    for test_case in test_suite:\n",
    "        test_result = test_runner.run(test_case)\n",
    "        \n",
    "   # Create a Spark session\n",
    "    spark = SparkSession.builder.appName(\"DataQuality\").getOrCreate()\n",
    "    \n",
    "    schema = [\"Table Name\", \"Status\", \"Test Type\", \"Date\"]\n",
    "    # Converting the test results to spark dataframe\n",
    "    results_df = pd.DataFrame(test_results)\n",
    "    results_df.to_csv(\"DataQuality.csv\", index=False)\n",
    "    # Write the individual Data Quality results to a CSV file\n",
    "    data_quality_df = spark.createDataFrame(results_df, schema=schema) \n",
    "    \n",
    "    \n",
    "     # Create a summary DataFrame to store load status\n",
    "    load_status_df = data_quality_df.groupby(\"Table Name\")\\\n",
    "        .agg(F.max(\"Status\").alias(\"Status\"), F.lit(\"Data Quality Checks\").alias(\"Test Type\"), F.current_date().alias(\"Date\"))\n",
    "\n",
    "    # Check if any row in the test results DataFrame has \"FAIL\" in the \"Status\" column\n",
    "    if \"FAIL\" in load_status_df.select(\"Status\").distinct().rdd.map(lambda x: x[0].upper()).collect():\n",
    "        # Set the status in the summary DataFrame to \"FAIL\"\n",
    "        load_status_df = load_status_df.withColumn(\"Status\", F.lit(\"FAIL\"))\n",
    "\n",
    "    # Define the file path for the load status CSV\n",
    "    csv_file_path = \"load_status\"\n",
    "    \n",
    "    # Check if the file already exists\n",
    "    #if os.path.isfile(f\"work/digital/logs/{csv_file_path}.csv\"):\n",
    "    if os.path.isfile(f\"{csv_file_path}.csv\"):\n",
    "        # Read the existing load status CSV directly\n",
    "        existing_load_status_df = spark.read.csv(f\"{csv_file_path}.csv\", header=True, inferSchema=True)\n",
    "\n",
    "        # Check if the date matches the current date\n",
    "        current_date = spark.sql(\"SELECT current_date() as current_date\").collect()[0][\"current_date\"]\n",
    "        matching_date_rows = existing_load_status_df.filter(existing_load_status_df[\"Date\"] == current_date)\n",
    "\n",
    "        if not matching_date_rows.isEmpty():\n",
    "            # Overwrite the existing values for the current date\n",
    "            existing_load_status_df = existing_load_status_df.filter(existing_load_status_df[\"Date\"] != current_date)\n",
    "            existing_load_status_df = existing_load_status_df.union(load_status_df)\n",
    "    else:\n",
    "        # The file doesn't exist, create it with the current load status\n",
    "        existing_load_status_df = load_status_df\n",
    "\n",
    "    # Write the updated or new load status to the CSV file\n",
    "    existing_load_status_df.show()\n",
    "    existing_load_status_dfp=existing_load_status_df.toPandas()\n",
    "    existing_load_status_dfp.to_csv(\"load_status.csv\", mode='a', index=False, header=not os.path.exists(\"load_status.csv\"))  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed12c22a-134f-4493-ac9e-c8bc07b2ed58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
