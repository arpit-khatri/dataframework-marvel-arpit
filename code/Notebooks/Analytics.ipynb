{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8782e0c-557c-454b-8db4-a07e0657f4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-18 15:54:33,283 - INFO - Running the SQL Scripts: analytics.sql\n",
      "2023-09-18 15:54:33,286 - INFO - Reading and executing SQL statements from /home/jovyan/work/digital/code/modelling.sql.\n",
      "2023-09-18 15:54:39,182 - INFO - Reading and executing SQL statements from /home/jovyan/work/digital/code/analytics.sql.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+----------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                 |comment|\n",
      "+----------------------------+----------------------------------------------------------+-------+\n",
      "|name                        |string                                                    |NULL   |\n",
      "|characterID                 |int                                                       |NULL   |\n",
      "|Alignment                   |string                                                    |NULL   |\n",
      "|Intelligence                |int                                                       |NULL   |\n",
      "|Strength                    |int                                                       |NULL   |\n",
      "|Speed                       |int                                                       |NULL   |\n",
      "|Durability                  |int                                                       |NULL   |\n",
      "|Power                       |int                                                       |NULL   |\n",
      "|Combat                      |int                                                       |NULL   |\n",
      "|Total                       |int                                                       |NULL   |\n",
      "|batch_id                    |string                                                    |NULL   |\n",
      "|load_date                   |string                                                    |NULL   |\n",
      "|                            |                                                          |       |\n",
      "|# Detailed Table Information|                                                          |       |\n",
      "|Catalog                     |spark_catalog                                             |       |\n",
      "|Database                    |db_sil_marvel                                             |       |\n",
      "|Table                       |char_stats_day_dly                                        |       |\n",
      "|Created Time                |Mon Sep 18 15:54:36 UTC 2023                              |       |\n",
      "|Last Access                 |UNKNOWN                                                   |       |\n",
      "|Created By                  |Spark 3.5.0                                               |       |\n",
      "|Type                        |EXTERNAL                                                  |       |\n",
      "|Provider                    |parquet                                                   |       |\n",
      "|Location                    |file:///home/jovyan/work/digital/target/char_stats_day_dly|       |\n",
      "+----------------------------+----------------------------------------------------------+-------+\n",
      "\n",
      "+---------------+-----------+---------+------------+--------+-----+----------+-----+------+-----+--------+--------------------------+\n",
      "|name           |characterID|Alignment|Intelligence|Strength|Speed|Durability|Power|Combat|Total|batch_id|load_date                 |\n",
      "+---------------+-----------+---------+------------+--------+-----+----------+-----+------+-----+--------+--------------------------+\n",
      "|Captain America|1009220    |good     |63          |19      |35   |56        |46   |100   |319  |101     |2023-09-18 15:45:31.110405|\n",
      "|Winter Soldier |1010740    |good     |56          |32      |35   |65        |60   |84    |332  |101     |2023-09-18 15:45:31.110405|\n",
      "|Nick Fury      |1009471    |good     |75          |11      |23   |42        |25   |100   |276  |101     |2023-09-18 15:45:31.110405|\n",
      "|Punisher       |1009515    |good     |50          |16      |23   |28        |22   |100   |239  |101     |2023-09-18 15:45:31.110405|\n",
      "|Red Skull      |1009535    |bad      |75          |10      |12   |14        |19   |80    |210  |101     |2023-09-18 15:45:31.110405|\n",
      "+---------------+-----------+---------+------------+--------+-----+----------+-----+------+-----+--------+--------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-18 15:54:45,133 - INFO - Data Analytics script completed successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+\n",
      "|total_heros|alignment|\n",
      "+-----------+---------+\n",
      "|5          |neutral  |\n",
      "|50         |bad      |\n",
      "|143        |good     |\n",
      "+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, current_timestamp\n",
    "from pyspark.sql.functions import *\n",
    "import datetime\n",
    "from datetime import datetime as dt\n",
    "import yaml\n",
    "\n",
    "def init_logger(log_path):\n",
    "    os.makedirs(log_path, exist_ok=True)\n",
    "    log_file_name = f\"analytics_{dt.now().strftime('%Y%m%d')}.log\"  # Include script name in the log file name\n",
    "    log_file_path = os.path.join(log_path, log_file_name)  # Full path to the log file\n",
    "\n",
    "    # Create a logger\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    # Create a single formatter for both the file handler and the console handler\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    # Create a handler for writing log messages to a file\n",
    "    file_handler = logging.FileHandler(log_file_path)\n",
    "    file_handler.setLevel(logging.INFO)\n",
    "    file_handler.setFormatter(formatter)\n",
    "\n",
    "    # Create a handler for writing log messages to the console\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "    console_handler.setFormatter(formatter)\n",
    "\n",
    "    # Add the handlers to the logger\n",
    "    logger.addHandler(file_handler)\n",
    "    logger.addHandler(console_handler)\n",
    "\n",
    "# Function to read the configuration from config.yaml\n",
    "def read_config():\n",
    "    try:\n",
    "        config_file_path = '/home/jovyan/work/digital/code/config.yaml'\n",
    "        if os.path.exists(config_file_path):\n",
    "            with open(config_file_path, 'r') as config_file:\n",
    "                config = yaml.safe_load(config_file)\n",
    "            return config\n",
    "        else:\n",
    "            logging.error(\"Error: config.yaml file not found.\")\n",
    "            return None\n",
    "    except yaml.YAMLError as e:\n",
    "        logging.error(f\"Error loading config.yaml: {e}\")\n",
    "        return None\n",
    "\n",
    "def execute_sql_script(sql_script_path, spark, db_name, table_name, parquet_path ):\n",
    "    \"\"\"\n",
    "    Executes SQL statements from a SQL script and return the results.\n",
    "\n",
    "    Args:\n",
    "        sql_script_path (str): Path to the SQL script.\n",
    "        spark (SparkSession): SparkSession object.\n",
    "        db_name (str): Name of the database.\n",
    "        table_name (str): Name of the table.\n",
    "        parquet_path (str): Path to the Parquet file.\n",
    "        script_name (str): Name of the script.\n",
    "\n",
    "    Returns:\n",
    "        list: List of DataFrames containing the results of SQL statements.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read and execute the SQL statements from the script\n",
    "        logging.info(f\"Reading and executing SQL statements from {sql_script_path}.\")\n",
    "        with open(sql_script_path, \"r\") as script_file:\n",
    "            script_content = script_file.read()  # Read the content of the file\n",
    "\n",
    "            # Replace placeholders with actual values\n",
    "            script_content = script_content.replace(\"${db_name}\", db_name)\n",
    "            script_content = script_content.replace(\"${table_name}\", table_name)\n",
    "            script_content = script_content.replace(\"${parquet_path}\", parquet_path)\n",
    "            sql_statements = script_content.split(\";\")\n",
    "\n",
    "            # Remove any empty statements in the SQL script\n",
    "            sql_statements = [statement.strip() for statement in sql_statements if statement.strip()]\n",
    "\n",
    "            # Execute each SQL statement separately\n",
    "            results = []\n",
    "            for statement in sql_statements:\n",
    "                if statement:\n",
    "                    result = spark.sql(statement)\n",
    "                    results.append(result)\n",
    "                    \n",
    "                    # Check if script_name contains \"analytics\" to determine whether to show the result\n",
    "                    if \"analytics\" in sql_script_path:\n",
    "                        result.show(50,truncate=False)  # Display the result\n",
    "                        \n",
    "            return results\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error: {str(e)}\", exc_info=True)\n",
    "        return []\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Read the configuration from config.yaml\n",
    "        config = read_config()\n",
    "\n",
    "        if config:\n",
    "            # Accessing the input_path from the config\n",
    "            output_path = config.get('output_path')\n",
    "            logging_path = config.get('log_path')\n",
    "            code_path = config.get('code_path')\n",
    "            db_name = config.get('database')['db_name'] \n",
    "            table_name = config.get('database')['table_name']\n",
    "            ddl_script = config.get('database')['ddl_script']\n",
    "            analytics_script = config.get('database')['analytics_script']\n",
    "            parquet_path = os.path.join(output_path, table_name)\n",
    "            \n",
    "            # Initialize the logger with the log_path from config\n",
    "            init_logger(logging_path)\n",
    "\n",
    "        else:\n",
    "            logging.error(\"Configuration not loaded. Please check the YAML file and its location.\")\n",
    " \n",
    "        # Creating a Spark session with the configured app name\n",
    "        spark = SparkSession.builder.appName(\"DataAnalytics\").getOrCreate()\n",
    "\n",
    "        # Defining the path to SQL script which will be used for creating Data Objects such as schema and tables\n",
    "        sql_ddl_script_path = os.path.join(code_path, ddl_script)\n",
    "        sql_analytics_script_path = os.path.join(code_path, analytics_script)\n",
    "        \n",
    "        # Execute the SQL script and get the results\n",
    "        logging.info(f\"Running the SQL Scripts: {analytics_script}\")\n",
    "        script_results = execute_sql_script(sql_ddl_script_path, spark, db_name, table_name, parquet_path)\n",
    "        script_results = execute_sql_script(sql_analytics_script_path, spark, db_name, table_name, parquet_path)\n",
    "            \n",
    "        logging.info(\"Data Analytics script completed successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error: {str(e)}\", exc_info=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958b485f-9036-422a-9e9d-5ca6ad331648",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
