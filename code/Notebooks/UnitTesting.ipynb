{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b155e57b-762e-4624-852d-6bcaa5c608c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 3.764s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit Test Table status update.\n",
      "+------------------+----------------------+------+-----------------+--------------------------+\n",
      "|Table Name        |Test Name             |Status|Reason           |Timestamp                 |\n",
      "+------------------+----------------------+------+-----------------+--------------------------+\n",
      "|char_stats_day_dly|name data_type        |PASS  |Expected Datatype|2023-09-18 20:58:09.700426|\n",
      "|char_stats_day_dly|characterID data_type |PASS  |Expected Datatype|2023-09-18 20:58:09.700447|\n",
      "|char_stats_day_dly|Alignment data_type   |PASS  |Expected Datatype|2023-09-18 20:58:09.700455|\n",
      "|char_stats_day_dly|Intelligence data_type|PASS  |Expected Datatype|2023-09-18 20:58:09.70046 |\n",
      "|char_stats_day_dly|Strength data_type    |PASS  |Expected Datatype|2023-09-18 20:58:09.700465|\n",
      "|char_stats_day_dly|Speed data_type       |PASS  |Expected Datatype|2023-09-18 20:58:09.700468|\n",
      "|char_stats_day_dly|Durability data_type  |PASS  |Expected Datatype|2023-09-18 20:58:09.700471|\n",
      "|char_stats_day_dly|Power data_type       |PASS  |Expected Datatype|2023-09-18 20:58:09.700481|\n",
      "|char_stats_day_dly|Combat data_type      |PASS  |Expected Datatype|2023-09-18 20:58:09.700485|\n",
      "|char_stats_day_dly|Total data_type       |PASS  |Expected Datatype|2023-09-18 20:58:09.700489|\n",
      "|char_stats_day_dly|batch_id data_type    |PASS  |Expected Datatype|2023-09-18 20:58:09.700496|\n",
      "|char_stats_day_dly|load_date data_type   |PASS  |Expected Datatype|2023-09-18 20:58:09.7005  |\n",
      "+------------------+----------------------+------+-----------------+--------------------------+\n",
      "\n",
      "Load Daily Status table update.\n",
      "+------------------+------+---------+----------+\n",
      "|        Table Name|Status|Test Type|      Date|\n",
      "+------------------+------+---------+----------+\n",
      "|char_stats_day_dly|  PASS| UnitTest|2023-09-18|\n",
      "+------------------+------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "import yaml\n",
    "import datetime\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "\n",
    "from datetime import datetime as dt\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, upper\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Function which initialize the logger with the date-appended log file name\n",
    "def init_logger(log_path):\n",
    "    os.makedirs(log_path, exist_ok=True)\n",
    "    log_file_name = f\"UnitTest_{datetime.datetime.now().strftime('%Y%m%d')}.log\"\n",
    "    logging.basicConfig(\n",
    "        filename=os.path.join(log_path, log_file_name),\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "    )\n",
    "\n",
    "class TestCharStats(unittest.TestCase):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Loading configuration from config.yaml\n",
    "        with open(\"/home/jovyan/work/digital/code/config.yaml\", \"r\") as config_file:\n",
    "            config = yaml.safe_load(config_file)\n",
    "\n",
    "        # Create a Spark session\n",
    "        self.spark = SparkSession.builder.appName(\"TestCharStats\").getOrCreate()\n",
    "\n",
    "        # Set the log level to ERROR or FATAL\n",
    "        self.spark.sparkContext.setLogLevel(\"ERROR\")  # You can change \"ERROR\" to \"FATAL\" if needed\n",
    "\n",
    "        # Assign configuration variables\n",
    "        self.columns_config = config[\"columns\"]\n",
    "        self.table_name = config[\"table_name\"]\n",
    "        self.output_path = config[\"output_path\"]\n",
    "        self.log_path = config[\"monitoring_path\"]\n",
    "        self.logging_path = config[\"log_path\"]\n",
    "        self.load_status_table = config[\"load_status_table\"]\n",
    "        self.absolute_table_name = f\"{self.output_path}/{self.table_name}\"\n",
    "\n",
    "        # Initialize logger\n",
    "        init_logger(self.logging_path)\n",
    "\n",
    "    # Function to log test results\n",
    "    def log_test_result(self, test_name, status, reason=\"\"):\n",
    "        timestamp = datetime.datetime.now()\n",
    "        # Creating a dictionary for the test result\n",
    "        result = {\n",
    "            \"Table Name\": self.table_name,\n",
    "            \"Test Name\": test_name,\n",
    "            \"Status\": status,\n",
    "            \"Reason\": reason,\n",
    "            \"Timestamp\": timestamp\n",
    "        }\n",
    "        # Append the result to the test_results list\n",
    "        test_results.append(result)\n",
    "\n",
    "    def test_column_data_types(self):\n",
    "        try:\n",
    "            # Logging the test start\n",
    "            logging.info(\"Starting the Unit Testing to match columns and datatypes.\")\n",
    "\n",
    "            # Loading the current data\n",
    "            df = self.spark.read.parquet(self.absolute_table_name)\n",
    "            logging.info(f\"Loaded data from {self.table_name}.\")\n",
    "\n",
    "            # Checking if column data types match the configuration\n",
    "            for column_config in self.columns_config:\n",
    "                column_name = column_config[\"name\"]\n",
    "                expected_data_type = column_config[\"type\"]\n",
    "                actual_data_type = df.schema[column_name].dataType.simpleString()\n",
    "                if actual_data_type == expected_data_type:\n",
    "                    self.log_test_result(f\"{column_name} data_type\", \"PASS\", \"Expected Datatype\")\n",
    "                    \n",
    "                else:\n",
    "                    self.log_test_result(f\"{column_name} data_type\", \"FAIL\", f\"Expected {expected_data_type}, but got {actual_data_type}\")\n",
    "\n",
    "            # Logging the test end\n",
    "            logging.info(\"Unit Testing to match columns and datatype.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in test_column_data_types: {str(e)}\")\n",
    "\n",
    "    # Function to log daily status\n",
    "    def daily_log_status(self, results_df):\n",
    "        try:\n",
    "            # Log the daily status start\n",
    "            logging.info(\"Starting the function to log the job satsus in daily_log_status table.\")\n",
    "\n",
    "            # Converting the test results to write into a file\n",
    "            results_df.to_csv(f\"{self.log_path}/UnitTest.csv\", index=False)\n",
    "            logging.info(f\"Saved test results to UnitTest table.\")\n",
    "\n",
    "            # Creating a Spark session and converting the test results to a Spark DataFrame\n",
    "            spark = SparkSession.builder.appName(\"UnitTest\").getOrCreate()\n",
    "            UnitTest_df = spark.createDataFrame(results_df)\n",
    "            print(\"Unit Test Table status update.\")\n",
    "            UnitTest_df.show(truncate=False)\n",
    "            logging.info(\"Creating a dataframe which will have daily_log_status.\")\n",
    "\n",
    "            # Creating a summary DataFrame to store load status\n",
    "            load_status_df = UnitTest_df \\\n",
    "                .groupBy(\"Table Name\") \\\n",
    "                .agg(F.when(F.expr(\"array_contains(collect_list(Status), 'FAIL')\"), \"FAIL\").otherwise(\"PASS\").alias(\"Status\")\\\n",
    "                    ,F.lit(\"UnitTest\").alias(\"Test Type\"), F.current_date().alias(\"Date\"))\n",
    "            \n",
    "            # Show the load_status_df\n",
    "            print(\"Load Daily Status table update.\")\n",
    "            load_status_df.show()\n",
    "\n",
    "            # Checking if any row in the test results DataFrame has \"FAIL\" in the \"Status\" column\n",
    "            if \"FAIL\" in load_status_df.select(\"Status\").distinct().rdd.map(lambda x: x[0].upper()).collect():\n",
    "                logging.warning(\"Load status set to FAIL due to Unit Testing failures.\")\n",
    "\n",
    "            else:\n",
    "                logging.info(\"Load status remains as PASS.\")\n",
    "\n",
    "            # Check if the log file already exists\n",
    "            if os.path.isfile(f\"{self.log_path}/{self.load_status_table}.csv\"):\n",
    "                # Reading the existing load status CSV directly\n",
    "                existing_load_status_df = spark.read.csv(f\"{self.log_path}/{self.load_status_table}.csv\", header=True, inferSchema=True)\n",
    "                final_load_status_df = existing_load_status_df.union(load_status_df).distinct()\n",
    "                final_load_status_dfp = final_load_status_df.toPandas()\n",
    "                final_load_status_dfp.to_csv(f\"{self.log_path}/{self.load_status_table}.csv\", index=False)\n",
    "                logging.info(f\"Appended the today's load status to table load_status.\")\n",
    "            else:\n",
    "                # if the file doesn't exist, creating it with the current load status\n",
    "                final_load_status_df = load_status_df\n",
    "                final_load_status_dfp = final_load_status_df.toPandas()\n",
    "                final_load_status_dfp.to_csv(f\"{self.log_path}/{self.load_status_table}.csv\", mode='a', index=False, header=not os.path.exists(f\"{self.log_path}/load_status.csv\"))\n",
    "                logging.info(f\"Created {self.log_path}/{self.load_status_table}.csv with current load status\")\n",
    "\n",
    "            # Log the daily status end\n",
    "            logging.info(\"Completed updating the daily_log_status for today's job status.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in daily_log_status: {str(e)}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        # Creating an empty list to store test results\n",
    "        test_results = []\n",
    "\n",
    "        # Creating a test suite\n",
    "        test_suite = unittest.TestLoader().loadTestsFromTestCase(TestCharStats)\n",
    "\n",
    "        # Running the tests\n",
    "        test_runner = unittest.TextTestRunner()\n",
    "\n",
    "        # Run each test and log the results\n",
    "        for test_case in test_suite:\n",
    "            test_result = test_runner.run(test_case)\n",
    "\n",
    "        # Converting the test results to write into a file\n",
    "        results_df = pd.DataFrame(test_results)\n",
    "\n",
    "        # Calling the daily_log_status function with test_results as an argument\n",
    "        test_instance = TestCharStats()\n",
    "        test_instance.daily_log_status(results_df)\n",
    "        logging.info(\"UnitTesting Job completed successfully.\")\n",
    "    except Exception as e:\n",
    "        # Handling fatal errors\n",
    "        error_message = str(e)\n",
    "        logging.error(f\"UnitTesting Job failed: {error_message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8810d8-c35e-4610-851e-3150388fc98c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
