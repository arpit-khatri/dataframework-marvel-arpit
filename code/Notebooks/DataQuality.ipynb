{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62f88805-d992-4755-9c02-13db8a1309a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 4.359s\n",
      "\n",
      "OK\n",
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 2.887s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+------+---------------------------------------------------+--------------------------+\n",
      "|Table Name        |Test Type          |Status|Reason                                             |Date                      |\n",
      "+------------------+-------------------+------+---------------------------------------------------+--------------------------+\n",
      "|char_stats_day_dly|test_count_increase|PASS  |Count increase percentage  is below threshold (10%)|2023-09-17 15:30:33.65893 |\n",
      "|char_stats_day_dly|test_primary_key   |FAIL  |Total of 1 duplicate primary key values found      |2023-09-17 15:30:36.548403|\n",
      "+------------------+-------------------+------+---------------------------------------------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "import yaml\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Function to initialize the logger with the date-appended log file name\n",
    "def init_logger(log_path):\n",
    "    os.makedirs(log_path, exist_ok=True)\n",
    "    log_file_name = f\"DataQuality_{datetime.datetime.now().strftime('%Y%m%d')}.log\"\n",
    "    logging.basicConfig(\n",
    "        filename=os.path.join(log_path, log_file_name),\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "    )\n",
    "\n",
    "class TestCharStats(unittest.TestCase):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Loading configuration from config.yaml\n",
    "        with open(\"/home/jovyan/work/digital/code/config.yaml\", \"r\") as config_file:\n",
    "            config = yaml.safe_load(config_file)\n",
    "\n",
    "        # Create a Spark session\n",
    "        self.spark = SparkSession.builder.appName(\"TestCharStats\").getOrCreate()\n",
    "\n",
    "        # Set the log level to ERROR or FATAL\n",
    "        self.spark.sparkContext.setLogLevel(\"ERROR\")  # You can change \"ERROR\" to \"FATAL\" if needed\n",
    "\n",
    "        # Assign configuration variables\n",
    "        self.table_name = config[\"table_name\"]\n",
    "        self.primary_key_columns = config[\"primary_key_columns\"]\n",
    "        self.threshold_percentage = config[\"threshold_percentage\"]\n",
    "        self.output_path = config[\"output_path\"]\n",
    "        self.log_path = config[\"monitoring_path\"]\n",
    "        self.logging_path = config[\"log_path\"]\n",
    "        self.absolute_table_name = f\"{self.output_path}/{self.table_name}\"\n",
    "        \n",
    "        # Initialize logger\n",
    "        init_logger(self.logging_path)\n",
    "\n",
    "    # Function to log test results\n",
    "    def log_test_result(self, test_name, status, reason=\"\"):\n",
    "        timestamp = datetime.datetime.now()\n",
    "        # Creating a dictionary for the test result\n",
    "        result = {\n",
    "            \"Table Name\": self.table_name,\n",
    "            \"Test Name\": test_name,\n",
    "            \"Status\": status,\n",
    "            \"Reason\": reason,\n",
    "            \"Timestamp\": timestamp\n",
    "        }\n",
    "        # Append the result to the test_results list\n",
    "        test_results.append(result)\n",
    "\n",
    "    def test_primary_key(self):\n",
    "        try:\n",
    "            # Log the test start\n",
    "            logging.info(\"Starting the process to to test primary_key duplicate check.\")\n",
    "\n",
    "            # Loading the current data\n",
    "            df = self.spark.read.parquet(self.absolute_table_name)\n",
    "            logging.info(f\"Loaded today's data from {self.table_name}.\")\n",
    "\n",
    "            # Checking primary key columns to make sure no duplicates\n",
    "            num_rows = df.count()\n",
    "            num_distinct_rows = df.select(*self.primary_key_columns).distinct().count()\n",
    "            if num_rows == num_distinct_rows:\n",
    "                self.log_test_result(\"test_primary_key\", \"PASS\")\n",
    "                logging.info(\"Duplicate Primary key test passed.\")\n",
    "            else:\n",
    "                count_duplicate_key = num_rows - num_distinct_rows\n",
    "                self.log_test_result(\"test_primary_key\", \"FAIL\", f\"Total of {count_duplicate_key} duplicate primary key values found\")\n",
    "                logging.warning(f\"Primary key test failed: Total of {count_duplicate_key} duplicate primary key values found\")\n",
    "\n",
    "            # Log the test end\n",
    "            logging.info(\"Completed the Primary key duplicate test.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in test_primary_key: {str(e)}\")\n",
    "\n",
    "    def test_count_increase(self):\n",
    "        try:\n",
    "            # Log the test start\n",
    "            logging.info(\"Starting the data quality check to monitor count_increase in today's load.\")\n",
    "\n",
    "            # Loading the today's and previous day data for the comparison of count\n",
    "            df_previous = self.spark.read.parquet(self.absolute_table_name)\n",
    "            df_current = self.spark.read.parquet(self.absolute_table_name)\n",
    "            logging.info(f\"Loaded data from yesterday data from table {self.table_name} for count comparison.\")\n",
    "\n",
    "            # Calculating the count increase percentage\n",
    "            count_previous = df_previous.count()\n",
    "            count_current = df_current.count()\n",
    "            increase_percentage = (count_current - count_previous) / count_previous * 100\n",
    "\n",
    "            if increase_percentage >= self.threshold_percentage:\n",
    "                self.log_test_result(\"test_count_increase\", \"FAIL\", f\"Count increase percentage ({increase_percentage}%) is more than threshold ({self.threshold_percentage}%)\")\n",
    "                logging.warning(f\"Count increase test failed: Count increase percentage ({increase_percentage}%) is more than threshold ({self.threshold_percentage}%)\")\n",
    "            else:\n",
    "                self.log_test_result(\"test_count_increase\", \"PASS\", f\"Count increase percentage  is below threshold ({self.threshold_percentage}%)\")\n",
    "                logging.info(f\"Count increase test passed: Count increase percentage is below threshold ({self.threshold_percentage}%)\")\n",
    "\n",
    "            # Log the test end\n",
    "            logging.info(\"Completed the data check for count_increase in today's load.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in data count_increase process: {str(e)}\")\n",
    "\n",
    "    def daily_log_status(self, results_df):\n",
    "        try:\n",
    "            # Log the daily status start\n",
    "            logging.info(\"Starting the function to log the job satsus in daily_log_status table.\")\n",
    "\n",
    "            # Write the individual Data Quality results to a CSV file\n",
    "            results_df.to_csv(f\"{self.log_path}/DataQuality.csv\", index=False)\n",
    "            logging.info(f\"Saved test results of  DataQuality table.\")\n",
    "\n",
    "            spark = SparkSession.builder.appName(\"DataQuality\").getOrCreate()\n",
    "            schema = [\"Table Name\", \"Test Type\", \"Status\", \"Reason\", \"Date\"]\n",
    "            data_quality_df = spark.createDataFrame(results_df, schema=schema)\n",
    "            data_quality_df.show(truncate=False)\n",
    "            logging.info(\"Displayed Data Quality results.\")\n",
    "\n",
    "            # Creating a summary DataFrame to store load status\n",
    "            load_status_df = data_quality_df.groupby(\"Table Name\") \\\n",
    "                .agg(F.max(\"Status\").alias(\"Status\"), F.lit(\"DataQuality\").alias(\"Test Type\"), F.current_date().alias(\"Date\"))\n",
    "\n",
    "            # Checking if any row in the test results DataFrame has \"FAIL\" in the \"Status\" column\n",
    "            if \"FAIL\" in load_status_df.select(\"Status\").distinct().rdd.map(lambda x: x[0].upper()).collect():\n",
    "                # Set the status in the summary DataFrame to \"FAIL\"\n",
    "                load_status_df = load_status_df.withColumn(\"Status\", F.lit(\"FAIL\"))\n",
    "                logging.warning(\"Load status set to FAIL due to Data Quality test failures.\")\n",
    "            else:\n",
    "                logging.info(\"Load status remains as PASS.\")\n",
    "\n",
    "            # Define the file path for the load status CSV\n",
    "            csv_file_path = \"load_status\"\n",
    "\n",
    "            # Check if the log file already exists\n",
    "            if os.path.isfile(f\"{self.log_path}/{csv_file_path}.csv\"):\n",
    "                # Reading the existing load status CSV directly\n",
    "                existing_load_status_df = spark.read.csv(f\"{self.log_path}/{csv_file_path}.csv\", header=True, inferSchema=True)\n",
    "                final_load_status_df = existing_load_status_df.union(load_status_df).distinct()\n",
    "                final_load_status_dfp = final_load_status_df.toPandas()\n",
    "                final_load_status_dfp.to_csv(f\"{self.log_path}/load_status.csv\", index=False)\n",
    "                logging.info(f\"Appended load status to existing load_status table.\")\n",
    "            else:\n",
    "                # if the file doesn't exist, creating it with the current load status\n",
    "                final_load_status_df = load_status_df\n",
    "                final_load_status_dfp = final_load_status_df.toPandas()\n",
    "                final_load_status_dfp.to_csv(f\"{self.log_path}/load_status.csv\", mode='a', index=False, header=not os.path.exists(f\"{self.log_path}/load_status.csv\"))\n",
    "                logging.info(f\"Created load_status with current load status\")\n",
    "\n",
    "            # Log the daily status end\n",
    "            logging.info(\"Completed the daily_log_status\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in daily_log_status: {str(e)}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Creating an empty list to store test results\n",
    "    test_results = []\n",
    "\n",
    "    # Creating a test suite\n",
    "    test_suite = unittest.TestLoader().loadTestsFromTestCase(TestCharStats)\n",
    "\n",
    "    # Running the tests\n",
    "    test_runner = unittest.TextTestRunner()\n",
    "\n",
    "    # Run each test and log the results\n",
    "    for test_case in test_suite:\n",
    "        test_result = test_runner.run(test_case)\n",
    "        \n",
    "     # Converting the test results to write into a file\n",
    "    results_df = pd.DataFrame(test_results)\n",
    "\n",
    "    # Call the daily_log_status function with test_results as an argument\n",
    "    test_instance = TestCharStats()\n",
    "    test_instance.daily_log_status(results_df)\n",
    "    logging.info(\"Data Quality Job completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec565e84-1335-4455-b05d-030d4abc2489",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
