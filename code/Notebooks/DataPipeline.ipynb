{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Input Path is /home/jovyan/work/digital/source\n",
      "2023-09-18 21:28:42,751 - INFO - Input Path is /home/jovyan/work/digital/source\n",
      "INFO:root:Target table Output Path is /home/jovyan/work/digital/target\n",
      "2023-09-18 21:28:42,756 - INFO - Target table Output Path is /home/jovyan/work/digital/target\n",
      "INFO:root:Reading characters.csv.\n",
      "2023-09-18 21:28:47,503 - INFO - Reading characters.csv.\n",
      "INFO:root:Reading characters_stats.csv.\n",
      "2023-09-18 21:28:54,433 - INFO - Reading characters_stats.csv.\n",
      "INFO:root:Joining character and Character Stats DataFrames.\n",
      "2023-09-18 21:28:55,125 - INFO - Joining character and Character Stats DataFrames.\n",
      "INFO:root:Adding audit columns.\n",
      "2023-09-18 21:28:55,202 - INFO - Adding audit columns.\n",
      "INFO:root:Saving DataFrame to Parquet file.\n",
      "2023-09-18 21:28:55,297 - INFO - Saving DataFrame to Parquet file.\n",
      "INFO:root:DataFrame saved to Parquet file: /home/jovyan/work/digital/target\n",
      "2023-09-18 21:28:58,594 - INFO - DataFrame saved to Parquet file: /home/jovyan/work/digital/target\n",
      "INFO:root:Running the SQL Script: modelling.sql\n",
      "2023-09-18 21:28:58,598 - INFO - Running the SQL Script: modelling.sql\n",
      "INFO:root:Reading and executing SQL statements from /home/jovyan/work/digital/code/modelling.sql.\n",
      "2023-09-18 21:28:58,602 - INFO - Reading and executing SQL statements from /home/jovyan/work/digital/code/modelling.sql.\n",
      "INFO:root:Target Table db_sil_marvel.char_stats_day_dly created Successfully.\n",
      "2023-09-18 21:28:59,719 - INFO - Target Table db_sil_marvel.char_stats_day_dly created Successfully.\n",
      "INFO:root:Data pipeline completed successfully.\n",
      "2023-09-18 21:28:59,721 - INFO - Data pipeline completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, current_timestamp\n",
    "import datetime\n",
    "from datetime import datetime as dt\n",
    "import yaml\n",
    "\n",
    "def init_logger(log_path):\n",
    "    os.makedirs(log_path, exist_ok=True)\n",
    "    log_file_name = f\"DataPipeline_{dt.now().strftime('%Y%m%d')}.log\"  \n",
    "    log_file_path = os.path.join(log_path, log_file_name)\n",
    "    # Creating a logger\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    # Creating a single formatter for both the file handler and the console handler\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    # Creating a handler for writing log messages to a file\n",
    "    file_handler = logging.FileHandler(log_file_path)\n",
    "    file_handler.setLevel(logging.INFO)\n",
    "    file_handler.setFormatter(formatter)\n",
    "\n",
    "    # Creating a handler for writing log messages to the console\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "    console_handler.setFormatter(formatter)\n",
    "\n",
    "    # Adding the handlers to the logger\n",
    "    logger.addHandler(file_handler)\n",
    "    logger.addHandler(console_handler)\n",
    "\n",
    "# Function to read the configuration from config.yaml\n",
    "def read_config():\n",
    "    try:\n",
    "        config_file_path = '/home/jovyan/work/digital/code/config.yaml'\n",
    "        if os.path.exists(config_file_path):\n",
    "            with open(config_file_path, 'r') as config_file:\n",
    "                config = yaml.safe_load(config_file)\n",
    "            return config\n",
    "        else:\n",
    "            logging.error(\"Error: config.yaml file not found.\")\n",
    "            return None\n",
    "    except yaml.YAMLError as e:\n",
    "        logging.error(f\"Error loading config.yaml: {e}\")\n",
    "        return None\n",
    "\n",
    "def execute_sql_script(sql_script_path, spark, db_name, table_name, parquet_path):\n",
    "    \"\"\"\n",
    "    Executes SQL statements from a SQL script and return the results.\n",
    "\n",
    "    Args:\n",
    "        sql_script_path (str): Path to the SQL script.\n",
    "        spark (SparkSession): SparkSession object.\n",
    "        db_name (str): Name of the database.\n",
    "        table_name (str): Name of the table.\n",
    "        parquet_path (str): Path to the Parquet file.\n",
    "\n",
    "    Returns:\n",
    "        list: List of DataFrames containing the results of SQL statements.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Reading and executing the SQL statements from the script\n",
    "        logging.info(f\"Reading and executing SQL statements from {sql_script_path}.\")\n",
    "        with open(sql_script_path, \"r\") as script_file:\n",
    "            script_content = script_file.read()  # Read the content of the file\n",
    "\n",
    "            # Replace placeholders with actual values\n",
    "            script_content = script_content.replace(\"${db_name}\", db_name)\n",
    "            script_content = script_content.replace(\"${table_name}\", table_name)\n",
    "            script_content = script_content.replace(\"${parquet_path}\", parquet_path)\n",
    "\n",
    "            sql_statements = script_content.split(\";\")\n",
    "\n",
    "            # Remove any empty statements in the SQL script\n",
    "            sql_statements = [statement.strip() for statement in sql_statements if statement.strip()]\n",
    "\n",
    "            # Execute each SQL statement separately\n",
    "            results = []\n",
    "            for statement in sql_statements:\n",
    "                if statement:\n",
    "                    result = spark.sql(statement)\n",
    "                    results.append(result)\n",
    "            return results\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error: {str(e)}\", exc_info=True)\n",
    "        return []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "            # Set the log level to ERROR or FATAL\n",
    "        self.spark.sparkContext.setLogLevel(\"ERROR\")  # You can change \"ERROR\" to \"FATAL\" if needed\n",
    "    try:\n",
    "        logging.info(\"Data pipeline started.\")\n",
    "        \n",
    "        # Reading the configuration from config.yaml\n",
    "        config = read_config()\n",
    "        \n",
    "        if config:\n",
    "            # Accessing the input_path from the config\n",
    "            input_path = config.get('input_path')\n",
    "            output_path = config.get('output_path')\n",
    "            logging_path = config.get('log_path')\n",
    "            code_path = config.get('code_path')\n",
    "            source_file1 = config.get('source_file1')\n",
    "            source_file2 = config.get('source_file2')\n",
    "            db_name = config.get('database')['db_name'] \n",
    "            table_name = config.get('database')['table_name']\n",
    "            ddl_script = config.get('database')['ddl_script']\n",
    "            parquet_path = os.path.join(output_path, table_name)\n",
    "            \n",
    "            # Initialize the logger with the log_path from config\n",
    "            #print(f\"log path is :{log_path}\")\n",
    "            init_logger(logging_path)\n",
    "\n",
    "            # Logging input and output paths\n",
    "            logging.info(f\"Input Path is {input_path}\")\n",
    "            logging.info(f\"Target table Output Path is {output_path}\")\n",
    "        else:\n",
    "            logging.error(\"Configuration not loaded. Please check the YAML file and its location.\")\n",
    " \n",
    "        # Creating a Spark session with the configured app name\n",
    "        spark = SparkSession.builder.appName(\"DataPipeline\").getOrCreate()\n",
    "\n",
    "        # Reading the Characters CSV file into a DataFrame\n",
    "        logging.info(f\"Reading {source_file1}.\")\n",
    "        df_char = spark.read.csv(os.path.join(input_path, source_file1), header=True, inferSchema=True)\n",
    "        \n",
    "        # Reading the Character Stats CSV file into a DataFrame\n",
    "        logging.info(f\"Reading {source_file2}.\")\n",
    "        df_stats = spark.read.csv(os.path.join(input_path, source_file2), header=True, inferSchema=True)\n",
    "        df_stats = df_stats.withColumnRenamed(\"Name\",\"name\")\n",
    "\n",
    "        # Joining the character & Character Stats files\n",
    "        logging.info(\"Joining character and Character Stats DataFrames.\")\n",
    "        df_char_stats = df_char.join(df_stats, on=\"name\", how=\"inner\")\n",
    "\n",
    "        # Adding the audit columns\n",
    "        logging.info(\"Adding audit columns.\")\n",
    "        df_char_stats = df_char_stats.withColumn(\"batch_id\", lit(\"101\"))\n",
    "        df_char_stats = df_char_stats.withColumn(\"load_date\", current_timestamp().cast(\"string\"))\n",
    "\n",
    "        # Saving the DataFrame to a Parquet file\n",
    "        logging.info(\"Saving DataFrame to Parquet file.\")\n",
    "        df_char_stats.write.parquet(os.path.join(output_path, \"char_stats_day_dly\"), mode=\"overwrite\")\n",
    "\n",
    "        # Print a message to confirm the file has been saved\n",
    "        logging.info(f\"DataFrame saved to Parquet file: {output_path}\")\n",
    "\n",
    "        # Defining the path to SQL script which will be used for creating Data Objects such as schema and tables\n",
    "        sql_script_path = os.path.join(code_path, ddl_script)\n",
    "        \n",
    "        # Execute the DDL SQL script and get the results\n",
    "        logging.info(f\"Running the SQL Script: {ddl_script}\")\n",
    "        script_results = execute_sql_script(sql_script_path, spark, db_name, table_name, parquet_path)\n",
    "        logging.info(f\"Target Table {db_name}.{table_name} created Successfully.\")\n",
    "        logging.info(\"Data pipeline completed successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handling fatal errors\n",
    "        error_message = str(e)\n",
    "        logging.error(f\"DataPipeline Job failed: {error_message}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
