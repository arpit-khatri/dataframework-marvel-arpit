{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b450efc2-01ef-4b77-820d-00dfb41fdacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.636s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------------------+------+------+--------------------------+\n",
      "|Table Name        |Test Name             |Status|Reason|Timestamp                 |\n",
      "+------------------+----------------------+------+------+--------------------------+\n",
      "|char_stats_day_dly|name data_type        |PASS  |      |2023-09-17 06:08:56.13323 |\n",
      "|char_stats_day_dly|characterID data_type |PASS  |      |2023-09-17 06:08:56.133243|\n",
      "|char_stats_day_dly|Alignment data_type   |PASS  |      |2023-09-17 06:08:56.133247|\n",
      "|char_stats_day_dly|Intelligence data_type|PASS  |      |2023-09-17 06:08:56.13325 |\n",
      "|char_stats_day_dly|Strength data_type    |PASS  |      |2023-09-17 06:08:56.133253|\n",
      "|char_stats_day_dly|Speed data_type       |PASS  |      |2023-09-17 06:08:56.133256|\n",
      "|char_stats_day_dly|Durability data_type  |PASS  |      |2023-09-17 06:08:56.133259|\n",
      "|char_stats_day_dly|Power data_type       |PASS  |      |2023-09-17 06:08:56.133268|\n",
      "|char_stats_day_dly|Combat data_type      |PASS  |      |2023-09-17 06:08:56.133272|\n",
      "|char_stats_day_dly|Total data_type       |PASS  |      |2023-09-17 06:08:56.133276|\n",
      "|char_stats_day_dly|batch_id data_type    |PASS  |      |2023-09-17 06:08:56.13328 |\n",
      "|char_stats_day_dly|load_date data_type   |PASS  |      |2023-09-17 06:08:56.133284|\n",
      "+------------------+----------------------+------+------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "import yaml\n",
    "import datetime\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import col, upper\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "\n",
    "# Define command-line arguments\n",
    "#parser = argparse.ArgumentParser(description=\"Unit test script for Spark DataFrame schema and data validation\")\n",
    "#parser.add_argument(\"--table_name\", required=True, help=\"Path to the target table\")\n",
    "\n",
    "class TestCharStats(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        # Parse command-line arguments\n",
    "        #args = parser.parse_args()\n",
    "        #self.table_name = args.table_name\n",
    "\n",
    "        # Loading configuration from config.yaml\n",
    "        with open(\"config.yaml\", \"r\") as config_file:\n",
    "            config = yaml.safe_load(config_file)\n",
    "\n",
    "        # Create a Spark session\n",
    "        self.spark = SparkSession.builder.appName(\"TestCharStats\").getOrCreate()\n",
    "\n",
    "        # Set the log level to ERROR or FATAL\n",
    "        self.spark.sparkContext.setLogLevel(\"ERROR\")  # You can change \"ERROR\" to \"FATAL\" if needed\n",
    "\n",
    "        # Assign configuration variables\n",
    "        self.columns_config = config[\"columns\"]\n",
    "        self.table_name = config[\"table_name\"]\n",
    "        self.output_path = config[\"output_path\"]\n",
    "        self.log_path = config[\"monitoring_path\"]\n",
    "        self.absolute_table_name =f\"{self.output_path}/{self.table_name}\"\n",
    "\n",
    "    def tearDown(self):\n",
    "        # Stop the Spark session\n",
    "        self.spark.stop()\n",
    "\n",
    "    def log_test_result(self, test_name, status, reason=\"\"):\n",
    "        timestamp = datetime.datetime.now()\n",
    "        # Creating a dictionary for the test result\n",
    "        result = {\n",
    "            \"Table Name\": self.table_name,\n",
    "            \"Test Name\": test_name,\n",
    "            \"Status\": status,\n",
    "            \"Reason\": reason,\n",
    "            \"Timestamp\": timestamp\n",
    "        }\n",
    "        # Append the result to the test_results list\n",
    "        test_results.append(result)\n",
    "\n",
    "    def test_column_data_types(self):\n",
    "        # Loading the current data\n",
    "        df = self.spark.read.parquet(self.absolute_table_name)\n",
    "\n",
    "        # Check if column data types match the configuration\n",
    "        for column_config in self.columns_config:\n",
    "            column_name = column_config[\"name\"]\n",
    "            expected_data_type = column_config[\"type\"]\n",
    "            actual_data_type = df.schema[column_name].dataType.simpleString()\n",
    "            if actual_data_type == expected_data_type:\n",
    "                self.log_test_result(f\"{column_name} data_type\", \"PASS\")\n",
    "            else:\n",
    "                self.log_test_result(f\"{column_name} data_type\", \"FAIL\", f\"Expected {expected_data_type}, but got {actual_data_type}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Creating an empty list to store test results\n",
    "    test_results = []\n",
    "\n",
    "    # Creating a test suite\n",
    "    test_suite = unittest.TestLoader().loadTestsFromTestCase(TestCharStats)\n",
    "\n",
    "    # Running the tests\n",
    "    test_runner = unittest.TextTestRunner()\n",
    "\n",
    "    # Run each test and log the results\n",
    "    for test_case in test_suite:\n",
    "        test_result = test_runner.run(test_case)\n",
    "\n",
    "    # Converting the test results to write into a file\n",
    "    results_df = pd.DataFrame(test_results)\n",
    "    results_df.to_csv(\"UnitTesting.csv\", index=False)\n",
    "\n",
    "    # Create a Spark session and converting the test results to spark dataframe\n",
    "    spark = SparkSession.builder.appName(\"UnitTesting\").getOrCreate()\n",
    "    #schema = [\"Table Name\", \"Status\", \"Test Type\", \"Date\"]\n",
    "\n",
    "    UnitTest_df = spark.createDataFrame(results_df)\n",
    "    UnitTest_df.show(truncate=False)\n",
    "\n",
    "     # Create a summary DataFrame to store load status\n",
    "    load_status_df = UnitTest_df.groupby(\"Table Name\")\\\n",
    "        .agg(F.max(\"Status\").alias(\"Status\"), F.lit(\"UnitTesting\").alias(\"Test Type\"), F.current_date().alias(\"Date\"))\n",
    "    # Check if any row in the test results DataFrame has \"FAIL\" in the \"Status\" column\n",
    "    if \"FAIL\" in load_status_df.select(\"Status\").distinct().rdd.map(lambda x: x[0].upper()).collect():\n",
    "        # Set the status in the summary DataFrame to \"FAIL\"\n",
    "        load_status_df = load_status_df.withColumn(\"Status\", F.lit(\"FAIL\"))\n",
    "    \n",
    "    # Define the file path for the load status CSV\n",
    "    csv_file_path = \"load_status\"\n",
    "    # Check if the file already exists\n",
    "    #if os.path.isfile(f\"work/digital/logs/{csv_file_path}.csv\"):\n",
    "    if os.path.isfile(f\"{csv_file_path}.csv\"):\n",
    "        ## Read the existing load status CSV directly\n",
    "        existing_load_status_df = spark.read.csv(f\"{csv_file_path}.csv\", header=True, inferSchema=True)\n",
    "        final_load_status_df = existing_load_status_df.union(load_status_df).distinct()\n",
    "        final_load_status_dfp=final_load_status_df.toPandas()\n",
    "        final_load_status_dfp.to_csv(\"load_status.csv\",index=False)\n",
    "    else:\n",
    "        # The file doesn't exist, create it with the current load status\n",
    "        final_load_status_df = load_status_df\n",
    "        final_load_status_dfp=final_load_status_df.toPandas()\n",
    "        final_load_status_dfp.to_csv(\"load_status.csv\", mode='a', index=False, header=not os.path.exists(\"load_status.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e41e7caa-49e5-48f1-86d8-ec6c02e9462a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.666s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------------------+------+------+--------------------------+\n",
      "|Table Name        |Test Name             |Status|Reason|Timestamp                 |\n",
      "+------------------+----------------------+------+------+--------------------------+\n",
      "|char_stats_day_dly|name data_type        |PASS  |      |2023-09-17 13:02:53.810336|\n",
      "|char_stats_day_dly|characterID data_type |PASS  |      |2023-09-17 13:02:53.810349|\n",
      "|char_stats_day_dly|Alignment data_type   |PASS  |      |2023-09-17 13:02:53.810354|\n",
      "|char_stats_day_dly|Intelligence data_type|PASS  |      |2023-09-17 13:02:53.810358|\n",
      "|char_stats_day_dly|Strength data_type    |PASS  |      |2023-09-17 13:02:53.810361|\n",
      "|char_stats_day_dly|Speed data_type       |PASS  |      |2023-09-17 13:02:53.810364|\n",
      "|char_stats_day_dly|Durability data_type  |PASS  |      |2023-09-17 13:02:53.810368|\n",
      "|char_stats_day_dly|Power data_type       |PASS  |      |2023-09-17 13:02:53.810378|\n",
      "|char_stats_day_dly|Combat data_type      |PASS  |      |2023-09-17 13:02:53.810384|\n",
      "|char_stats_day_dly|Total data_type       |PASS  |      |2023-09-17 13:02:53.810388|\n",
      "|char_stats_day_dly|batch_id data_type    |PASS  |      |2023-09-17 13:02:53.810392|\n",
      "|char_stats_day_dly|load_date data_type   |PASS  |      |2023-09-17 13:02:53.810397|\n",
      "+------------------+----------------------+------+------+--------------------------+\n",
      "\n",
      "Job completed successfully\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "import yaml\n",
    "import datetime\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import col, upper\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "\n",
    "# Define command-line arguments\n",
    "#parser = argparse.ArgumentParser(description=\"Unit test script for Spark DataFrame schema and data validation\")\n",
    "#parser.add_argument(\"--table_name\", required=True, help=\"Path to the target table\")\n",
    "\n",
    "class TestCharStats(unittest.TestCase):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Loading configuration from config.yaml\n",
    "        with open(\"config.yaml\", \"r\") as config_file:\n",
    "            config = yaml.safe_load(config_file)\n",
    "\n",
    "        # Create a Spark session\n",
    "        self.spark = SparkSession.builder.appName(\"TestCharStats\").getOrCreate()\n",
    "\n",
    "        # Set the log level to ERROR or FATAL\n",
    "        self.spark.sparkContext.setLogLevel(\"ERROR\")  # You can change \"ERROR\" to \"FATAL\" if needed\n",
    "\n",
    "        # Assign configuration variables\n",
    "        self.columns_config = config[\"columns\"]\n",
    "        self.table_name = config[\"table_name\"]\n",
    "        self.output_path = config[\"output_path\"]\n",
    "        self.log_path = config[\"monitoring_path\"]\n",
    "        self.absolute_table_name = f\"{self.output_path}/{self.table_name}\"\n",
    "        \n",
    "\n",
    "    def log_test_result(self, test_name, status, reason=\"\"):\n",
    "        timestamp = datetime.datetime.now()\n",
    "        # Creating a dictionary for the test result\n",
    "        result = {\n",
    "            \"Table Name\": self.table_name,\n",
    "            \"Test Name\": test_name,\n",
    "            \"Status\": status,\n",
    "            \"Reason\": reason,\n",
    "            \"Timestamp\": timestamp\n",
    "        }\n",
    "        # Append the result to the test_results list\n",
    "        test_results.append(result)\n",
    "\n",
    "    def test_column_data_types(self):\n",
    "        # Loading the current data\n",
    "        df = self.spark.read.parquet(self.absolute_table_name)\n",
    "\n",
    "        # Check if column data types match the configuration\n",
    "        for column_config in self.columns_config:\n",
    "            column_name = column_config[\"name\"]\n",
    "            expected_data_type = column_config[\"type\"]\n",
    "            actual_data_type = df.schema[column_name].dataType.simpleString()\n",
    "            if actual_data_type == expected_data_type:\n",
    "                self.log_test_result(f\"{column_name} data_type\", \"PASS\")\n",
    "            else:\n",
    "                self.log_test_result(f\"{column_name} data_type\", \"FAIL\", f\"Expected {expected_data_type}, but got {actual_data_type}\")\n",
    "    def daily_log_status(self, results_df):\n",
    "        # Converting the test results to write into a file\n",
    "        results_df.to_csv(f\"{self.log_path}/UnitTesting.csv\", index=False)\n",
    "\n",
    "        # Creating a Spark session and converting the test results to spark dataframe\n",
    "        spark = SparkSession.builder.appName(\"UnitTesting\").getOrCreate()\n",
    "        UnitTest_df = spark.createDataFrame(results_df)\n",
    "        UnitTest_df.show(truncate=False)\n",
    "\n",
    "        # Creating a summary DataFrame to store load status\n",
    "        load_status_df = UnitTest_df.groupby(\"Table Name\") \\\n",
    "            .agg(F.max(\"Status\").alias(\"Status\"), F.lit(\"UnitTesting\").alias(\"Test Type\"), F.current_date().alias(\"Date\"))\n",
    "\n",
    "        # Checking if any row in the test results DataFrame has \"FAIL\" in the \"Status\" column\n",
    "        if \"FAIL\" in load_status_df.select(\"Status\").distinct().rdd.map(lambda x: x[0].upper()).collect():\n",
    "            # Set the status in the summary DataFrame to \"FAIL\"\n",
    "            load_status_df = load_status_df.withColumn(\"Status\", F.lit(\"FAIL\"))\n",
    "\n",
    "        # Define the file path for the load status CSV\n",
    "        csv_file_path = \"load_status\"\n",
    "\n",
    "        # Check if the log file already exists\n",
    "        if os.path.isfile(f\"{self.log_path}/{csv_file_path}.csv\"):\n",
    "            # Reading the existing load status CSV directly\n",
    "            existing_load_status_df = spark.read.csv(f\"{self.log_path}/{csv_file_path}.csv\", header=True, inferSchema=True)\n",
    "            final_load_status_df = existing_load_status_df.union(load_status_df).distinct()\n",
    "            final_load_status_dfp = final_load_status_df.toPandas()\n",
    "            final_load_status_dfp.to_csv(f\"{self.log_path}/load_status.csv\", index=False)\n",
    "        else:\n",
    "            # if the file doesn't exist, creating it with the current load status\n",
    "            final_load_status_df = load_status_df\n",
    "            final_load_status_dfp = final_load_status_df.toPandas()\n",
    "            final_load_status_dfp.to_csv(f\"{self.log_path}/load_status.csv\", mode='a', index=False, header=not os.path.exists(f\"{self.log_path}/load_status.csv\"))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Creating an empty list to store test results\n",
    "    test_results = []\n",
    "\n",
    "    # Creating a test suite\n",
    "    test_suite = unittest.TestLoader().loadTestsFromTestCase(TestCharStats)\n",
    "\n",
    "    # Running the tests\n",
    "    test_runner = unittest.TextTestRunner()\n",
    "\n",
    "    # Run each test and log the results\n",
    "    for test_case in test_suite:\n",
    "        test_result = test_runner.run(test_case)\n",
    "\n",
    "    # Converting the test results to write into a file\n",
    "    results_df = pd.DataFrame(test_results)\n",
    "    #results_df.to_csv(\"UnitTesting.csv\", index=False)\n",
    "\n",
    "    # Call the daily_log_status function with test_results as an argument\n",
    "    test_instance = TestCharStats()\n",
    "    test_instance.daily_log_status(results_df)\n",
    "    print(\"Job completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b155e57b-762e-4624-852d-6bcaa5c608c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
