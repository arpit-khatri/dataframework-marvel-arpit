{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b450efc2-01ef-4b77-820d-00dfb41fdacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.636s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------------------+------+------+--------------------------+\n",
      "|Table Name        |Test Name             |Status|Reason|Timestamp                 |\n",
      "+------------------+----------------------+------+------+--------------------------+\n",
      "|char_stats_day_dly|name data_type        |PASS  |      |2023-09-17 06:08:56.13323 |\n",
      "|char_stats_day_dly|characterID data_type |PASS  |      |2023-09-17 06:08:56.133243|\n",
      "|char_stats_day_dly|Alignment data_type   |PASS  |      |2023-09-17 06:08:56.133247|\n",
      "|char_stats_day_dly|Intelligence data_type|PASS  |      |2023-09-17 06:08:56.13325 |\n",
      "|char_stats_day_dly|Strength data_type    |PASS  |      |2023-09-17 06:08:56.133253|\n",
      "|char_stats_day_dly|Speed data_type       |PASS  |      |2023-09-17 06:08:56.133256|\n",
      "|char_stats_day_dly|Durability data_type  |PASS  |      |2023-09-17 06:08:56.133259|\n",
      "|char_stats_day_dly|Power data_type       |PASS  |      |2023-09-17 06:08:56.133268|\n",
      "|char_stats_day_dly|Combat data_type      |PASS  |      |2023-09-17 06:08:56.133272|\n",
      "|char_stats_day_dly|Total data_type       |PASS  |      |2023-09-17 06:08:56.133276|\n",
      "|char_stats_day_dly|batch_id data_type    |PASS  |      |2023-09-17 06:08:56.13328 |\n",
      "|char_stats_day_dly|load_date data_type   |PASS  |      |2023-09-17 06:08:56.133284|\n",
      "+------------------+----------------------+------+------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "import yaml\n",
    "import datetime\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import col, upper\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "\n",
    "# Define command-line arguments\n",
    "#parser = argparse.ArgumentParser(description=\"Unit test script for Spark DataFrame schema and data validation\")\n",
    "#parser.add_argument(\"--table_name\", required=True, help=\"Path to the target table\")\n",
    "\n",
    "class TestCharStats(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        # Parse command-line arguments\n",
    "        #args = parser.parse_args()\n",
    "        #self.table_name = args.table_name\n",
    "\n",
    "        # Loading configuration from config.yaml\n",
    "        with open(\"config.yaml\", \"r\") as config_file:\n",
    "            config = yaml.safe_load(config_file)\n",
    "\n",
    "        # Create a Spark session\n",
    "        self.spark = SparkSession.builder.appName(\"TestCharStats\").getOrCreate()\n",
    "\n",
    "        # Set the log level to ERROR or FATAL\n",
    "        self.spark.sparkContext.setLogLevel(\"ERROR\")  # You can change \"ERROR\" to \"FATAL\" if needed\n",
    "\n",
    "        # Assign configuration variables\n",
    "        self.columns_config = config[\"columns\"]\n",
    "        self.table_name = config[\"table_name\"]\n",
    "        self.output_path = config[\"output_path\"]\n",
    "        self.log_path = config[\"monitoring_path\"]\n",
    "        self.absolute_table_name =f\"{self.output_path}/{self.table_name}\"\n",
    "\n",
    "    def tearDown(self):\n",
    "        # Stop the Spark session\n",
    "        self.spark.stop()\n",
    "\n",
    "    def log_test_result(self, test_name, status, reason=\"\"):\n",
    "        timestamp = datetime.datetime.now()\n",
    "        # Creating a dictionary for the test result\n",
    "        result = {\n",
    "            \"Table Name\": self.table_name,\n",
    "            \"Test Name\": test_name,\n",
    "            \"Status\": status,\n",
    "            \"Reason\": reason,\n",
    "            \"Timestamp\": timestamp\n",
    "        }\n",
    "        # Append the result to the test_results list\n",
    "        test_results.append(result)\n",
    "\n",
    "    def test_column_data_types(self):\n",
    "        # Loading the current data\n",
    "        df = self.spark.read.parquet(self.absolute_table_name)\n",
    "\n",
    "        # Check if column data types match the configuration\n",
    "        for column_config in self.columns_config:\n",
    "            column_name = column_config[\"name\"]\n",
    "            expected_data_type = column_config[\"type\"]\n",
    "            actual_data_type = df.schema[column_name].dataType.simpleString()\n",
    "            if actual_data_type == expected_data_type:\n",
    "                self.log_test_result(f\"{column_name} data_type\", \"PASS\")\n",
    "            else:\n",
    "                self.log_test_result(f\"{column_name} data_type\", \"FAIL\", f\"Expected {expected_data_type}, but got {actual_data_type}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Creating an empty list to store test results\n",
    "    test_results = []\n",
    "\n",
    "    # Creating a test suite\n",
    "    test_suite = unittest.TestLoader().loadTestsFromTestCase(TestCharStats)\n",
    "\n",
    "    # Running the tests\n",
    "    test_runner = unittest.TextTestRunner()\n",
    "\n",
    "    # Run each test and log the results\n",
    "    for test_case in test_suite:\n",
    "        test_result = test_runner.run(test_case)\n",
    "\n",
    "    # Converting the test results to write into a file\n",
    "    results_df = pd.DataFrame(test_results)\n",
    "    results_df.to_csv(\"UnitTesting.csv\", index=False)\n",
    "\n",
    "    # Create a Spark session and converting the test results to spark dataframe\n",
    "    spark = SparkSession.builder.appName(\"UnitTesting\").getOrCreate()\n",
    "    #schema = [\"Table Name\", \"Status\", \"Test Type\", \"Date\"]\n",
    "\n",
    "    UnitTest_df = spark.createDataFrame(results_df)\n",
    "    UnitTest_df.show(truncate=False)\n",
    "\n",
    "     # Create a summary DataFrame to store load status\n",
    "    load_status_df = UnitTest_df.groupby(\"Table Name\")\\\n",
    "        .agg(F.max(\"Status\").alias(\"Status\"), F.lit(\"UnitTesting\").alias(\"Test Type\"), F.current_date().alias(\"Date\"))\n",
    "    # Check if any row in the test results DataFrame has \"FAIL\" in the \"Status\" column\n",
    "    if \"FAIL\" in load_status_df.select(\"Status\").distinct().rdd.map(lambda x: x[0].upper()).collect():\n",
    "        # Set the status in the summary DataFrame to \"FAIL\"\n",
    "        load_status_df = load_status_df.withColumn(\"Status\", F.lit(\"FAIL\"))\n",
    "    \n",
    "    # Define the file path for the load status CSV\n",
    "    csv_file_path = \"load_status\"\n",
    "    # Check if the file already exists\n",
    "    #if os.path.isfile(f\"work/digital/logs/{csv_file_path}.csv\"):\n",
    "    if os.path.isfile(f\"{csv_file_path}.csv\"):\n",
    "        ## Read the existing load status CSV directly\n",
    "        existing_load_status_df = spark.read.csv(f\"{csv_file_path}.csv\", header=True, inferSchema=True)\n",
    "        final_load_status_df = existing_load_status_df.union(load_status_df).distinct()\n",
    "        final_load_status_dfp=final_load_status_df.toPandas()\n",
    "        final_load_status_dfp.to_csv(\"load_status.csv\",index=False)\n",
    "    else:\n",
    "        # The file doesn't exist, create it with the current load status\n",
    "        final_load_status_df = load_status_df\n",
    "        final_load_status_dfp=final_load_status_df.toPandas()\n",
    "        final_load_status_dfp.to_csv(\"load_status.csv\", mode='a', index=False, header=not os.path.exists(\"load_status.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e41e7caa-49e5-48f1-86d8-ec6c02e9462a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.666s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------------------+------+------+--------------------------+\n",
      "|Table Name        |Test Name             |Status|Reason|Timestamp                 |\n",
      "+------------------+----------------------+------+------+--------------------------+\n",
      "|char_stats_day_dly|name data_type        |PASS  |      |2023-09-17 13:02:53.810336|\n",
      "|char_stats_day_dly|characterID data_type |PASS  |      |2023-09-17 13:02:53.810349|\n",
      "|char_stats_day_dly|Alignment data_type   |PASS  |      |2023-09-17 13:02:53.810354|\n",
      "|char_stats_day_dly|Intelligence data_type|PASS  |      |2023-09-17 13:02:53.810358|\n",
      "|char_stats_day_dly|Strength data_type    |PASS  |      |2023-09-17 13:02:53.810361|\n",
      "|char_stats_day_dly|Speed data_type       |PASS  |      |2023-09-17 13:02:53.810364|\n",
      "|char_stats_day_dly|Durability data_type  |PASS  |      |2023-09-17 13:02:53.810368|\n",
      "|char_stats_day_dly|Power data_type       |PASS  |      |2023-09-17 13:02:53.810378|\n",
      "|char_stats_day_dly|Combat data_type      |PASS  |      |2023-09-17 13:02:53.810384|\n",
      "|char_stats_day_dly|Total data_type       |PASS  |      |2023-09-17 13:02:53.810388|\n",
      "|char_stats_day_dly|batch_id data_type    |PASS  |      |2023-09-17 13:02:53.810392|\n",
      "|char_stats_day_dly|load_date data_type   |PASS  |      |2023-09-17 13:02:53.810397|\n",
      "+------------------+----------------------+------+------+--------------------------+\n",
      "\n",
      "Job completed successfully\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "import yaml\n",
    "import datetime\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import col, upper\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Define command-line arguments\n",
    "#parser = argparse.ArgumentParser(description=\"Unit test script for Spark DataFrame schema and data validation\")\n",
    "#parser.add_argument(\"--table_name\", required=True, help=\"Path to the target table\")\n",
    "\n",
    "class TestCharStats(unittest.TestCase):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Loading configuration from config.yaml\n",
    "        with open(\"config.yaml\", \"r\") as config_file:\n",
    "            config = yaml.safe_load(config_file)\n",
    "\n",
    "        # Create a Spark session\n",
    "        self.spark = SparkSession.builder.appName(\"TestCharStats\").getOrCreate()\n",
    "\n",
    "        # Set the log level to ERROR or FATAL\n",
    "        self.spark.sparkContext.setLogLevel(\"ERROR\")  # You can change \"ERROR\" to \"FATAL\" if needed\n",
    "\n",
    "        # Assign configuration variables\n",
    "        self.columns_config = config[\"columns\"]\n",
    "        self.table_name = config[\"table_name\"]\n",
    "        self.output_path = config[\"output_path\"]\n",
    "        self.log_path = config[\"monitoring_path\"]\n",
    "        self.absolute_table_name = f\"{self.output_path}/{self.table_name}\"\n",
    "        \n",
    "\n",
    "    def log_test_result(self, test_name, status, reason=\"\"):\n",
    "        timestamp = datetime.datetime.now()\n",
    "        # Creating a dictionary for the test result\n",
    "        result = {\n",
    "            \"Table Name\": self.table_name,\n",
    "            \"Test Name\": test_name,\n",
    "            \"Status\": status,\n",
    "            \"Reason\": reason,\n",
    "            \"Timestamp\": timestamp\n",
    "        }\n",
    "        # Append the result to the test_results list\n",
    "        test_results.append(result)\n",
    "\n",
    "    def test_column_data_types(self):\n",
    "        # Loading the current data\n",
    "        df = self.spark.read.parquet(self.absolute_table_name)\n",
    "\n",
    "        # Check if column data types match the configuration\n",
    "        for column_config in self.columns_config:\n",
    "            column_name = column_config[\"name\"]\n",
    "            expected_data_type = column_config[\"type\"]\n",
    "            actual_data_type = df.schema[column_name].dataType.simpleString()\n",
    "            if actual_data_type == expected_data_type:\n",
    "                self.log_test_result(f\"{column_name} data_type\", \"PASS\")\n",
    "            else:\n",
    "                self.log_test_result(f\"{column_name} data_type\", \"FAIL\", f\"Expected {expected_data_type}, but got {actual_data_type}\")\n",
    "    def daily_log_status(self, results_df):\n",
    "        # Converting the test results to write into a file\n",
    "        results_df.to_csv(f\"{self.log_path}/UnitTesting.csv\", index=False)\n",
    "\n",
    "        # Creating a Spark session and converting the test results to spark dataframe\n",
    "        spark = SparkSession.builder.appName(\"UnitTesting\").getOrCreate()\n",
    "        UnitTest_df = spark.createDataFrame(results_df)\n",
    "        UnitTest_df.show(truncate=False)\n",
    "\n",
    "        # Creating a summary DataFrame to store load status\n",
    "        load_status_df = UnitTest_df.groupby(\"Table Name\") \\\n",
    "            .agg(F.max(\"Status\").alias(\"Status\"), F.lit(\"UnitTesting\").alias(\"Test Type\"), F.current_date().alias(\"Date\"))\n",
    "\n",
    "        # Checking if any row in the test results DataFrame has \"FAIL\" in the \"Status\" column\n",
    "        if \"FAIL\" in load_status_df.select(\"Status\").distinct().rdd.map(lambda x: x[0].upper()).collect():\n",
    "            # Set the status in the summary DataFrame to \"FAIL\"\n",
    "            load_status_df = load_status_df.withColumn(\"Status\", F.lit(\"FAIL\"))\n",
    "\n",
    "        # Define the file path for the load status CSV\n",
    "        csv_file_path = \"load_status\"\n",
    "\n",
    "        # Check if the log file already exists\n",
    "        if os.path.isfile(f\"{self.log_path}/{csv_file_path}.csv\"):\n",
    "            # Reading the existing load status CSV directly\n",
    "            existing_load_status_df = spark.read.csv(f\"{self.log_path}/{csv_file_path}.csv\", header=True, inferSchema=True)\n",
    "            final_load_status_df = existing_load_status_df.union(load_status_df).distinct()\n",
    "            final_load_status_dfp = final_load_status_df.toPandas()\n",
    "            final_load_status_dfp.to_csv(f\"{self.log_path}/load_status.csv\", index=False)\n",
    "        else:\n",
    "            # if the file doesn't exist, creating it with the current load status\n",
    "            final_load_status_df = load_status_df\n",
    "            final_load_status_dfp = final_load_status_df.toPandas()\n",
    "            final_load_status_dfp.to_csv(f\"{self.log_path}/load_status.csv\", mode='a', index=False, header=not os.path.exists(f\"{self.log_path}/load_status.csv\"))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Creating an empty list to store test results\n",
    "    test_results = []\n",
    "\n",
    "    # Creating a test suite\n",
    "    test_suite = unittest.TestLoader().loadTestsFromTestCase(TestCharStats)\n",
    "\n",
    "    # Running the tests\n",
    "    test_runner = unittest.TextTestRunner()\n",
    "\n",
    "    # Run each test and log the results\n",
    "    for test_case in test_suite:\n",
    "        test_result = test_runner.run(test_case)\n",
    "\n",
    "    # Converting the test results to write into a file\n",
    "    results_df = pd.DataFrame(test_results)\n",
    "\n",
    "    # Call the daily_log_status function with test_results as an argument\n",
    "    test_instance = TestCharStats()\n",
    "    test_instance.daily_log_status(results_df)\n",
    "    print(\"Job completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b155e57b-762e-4624-852d-6bcaa5c608c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 2.749s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------------------+------+------+--------------------------+\n",
      "|Table Name        |Test Name             |Status|Reason|Timestamp                 |\n",
      "+------------------+----------------------+------+------+--------------------------+\n",
      "|char_stats_day_dly|name data_type        |PASS  |      |2023-09-17 14:03:01.738409|\n",
      "|char_stats_day_dly|characterID data_type |PASS  |      |2023-09-17 14:03:01.738433|\n",
      "|char_stats_day_dly|Alignment data_type   |PASS  |      |2023-09-17 14:03:01.738445|\n",
      "|char_stats_day_dly|Intelligence data_type|PASS  |      |2023-09-17 14:03:01.738452|\n",
      "|char_stats_day_dly|Strength data_type    |PASS  |      |2023-09-17 14:03:01.738455|\n",
      "|char_stats_day_dly|Speed data_type       |PASS  |      |2023-09-17 14:03:01.738459|\n",
      "|char_stats_day_dly|Durability data_type  |PASS  |      |2023-09-17 14:03:01.738463|\n",
      "|char_stats_day_dly|Power data_type       |PASS  |      |2023-09-17 14:03:01.73848 |\n",
      "|char_stats_day_dly|Combat data_type      |PASS  |      |2023-09-17 14:03:01.738487|\n",
      "|char_stats_day_dly|Total data_type       |PASS  |      |2023-09-17 14:03:01.738491|\n",
      "|char_stats_day_dly|batch_id data_type    |PASS  |      |2023-09-17 14:03:01.738496|\n",
      "|char_stats_day_dly|load_date data_type   |PASS  |      |2023-09-17 14:03:01.7385  |\n",
      "+------------------+----------------------+------+------+--------------------------+\n",
      "\n",
      "UnitTesting Job completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "import yaml\n",
    "import datetime\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import col, upper\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Function to initialize the logger with the date-appended log file name\n",
    "def init_logger(log_path):\n",
    "    os.makedirs(log_path, exist_ok=True)\n",
    "    log_file_name = f\"UnitTest_{datetime.datetime.now().strftime('%Y%m%d')}.log\"\n",
    "    logging.basicConfig(\n",
    "        filename=os.path.join(log_path, log_file_name),\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "    )\n",
    "\n",
    "# Define command-line arguments\n",
    "# parser = argparse.ArgumentParser(description=\"Unit test script for Spark DataFrame schema and data validation\")\n",
    "# parser.add_argument(\"--table_name\", required=True, help=\"Path to the target table\")\n",
    "\n",
    "class TestCharStats(unittest.TestCase):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Loading configuration from config.yaml\n",
    "        with open(\"config.yaml\", \"r\") as config_file:\n",
    "            config = yaml.safe_load(config_file)\n",
    "\n",
    "        # Create a Spark session\n",
    "        self.spark = SparkSession.builder.appName(\"TestCharStats\").getOrCreate()\n",
    "\n",
    "        # Set the log level to ERROR or FATAL\n",
    "        self.spark.sparkContext.setLogLevel(\"ERROR\")  # You can change \"ERROR\" to \"FATAL\" if needed\n",
    "\n",
    "        # Assign configuration variables\n",
    "        self.columns_config = config[\"columns\"]\n",
    "        self.table_name = config[\"table_name\"]\n",
    "        self.output_path = config[\"output_path\"]\n",
    "        self.log_path = config[\"monitoring_path\"]\n",
    "        self.logging_path = config[\"log_path\"]\n",
    "        self.absolute_table_name = f\"{self.output_path}/{self.table_name}\"\n",
    "\n",
    "        # Initialize logger\n",
    "        init_logger(self.logging_path)\n",
    "\n",
    "    # Function to log test results\n",
    "    def log_test_result(self, test_name, status, reason=\"\"):\n",
    "        timestamp = datetime.datetime.now()\n",
    "        # Creating a dictionary for the test result\n",
    "        result = {\n",
    "            \"Table Name\": self.table_name,\n",
    "            \"Test Name\": test_name,\n",
    "            \"Status\": status,\n",
    "            \"Reason\": reason,\n",
    "            \"Timestamp\": timestamp\n",
    "        }\n",
    "        # Append the result to the test_results list\n",
    "        test_results.append(result)\n",
    "\n",
    "    def test_column_data_types(self):\n",
    "        try:\n",
    "            # Log the test start\n",
    "            logging.info(\"Starting the Unit Testing to match columns and datatypes.\")\n",
    "\n",
    "            # Loading the current data\n",
    "            df = self.spark.read.parquet(self.absolute_table_name)\n",
    "            logging.info(f\"Loaded data from {self.table_name}.\")\n",
    "\n",
    "            # Check if column data types match the configuration\n",
    "            for column_config in self.columns_config:\n",
    "                column_name = column_config[\"name\"]\n",
    "                expected_data_type = column_config[\"type\"]\n",
    "                actual_data_type = df.schema[column_name].dataType.simpleString()\n",
    "                if actual_data_type == expected_data_type:\n",
    "                    self.log_test_result(f\"{column_name} data_type\", \"PASS\")\n",
    "                else:\n",
    "                    self.log_test_result(f\"{column_name} data_type\", \"FAIL\", f\"Expected {expected_data_type}, but got {actual_data_type}\")\n",
    "\n",
    "            # Log the test end\n",
    "            logging.info(\"Unit Testing to match columns and datatype.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in test_column_data_types: {str(e)}\")\n",
    "\n",
    "    # Function to log daily status\n",
    "    def daily_log_status(self, results_df):\n",
    "        try:\n",
    "            # Log the daily status start\n",
    "            logging.info(\"Starting the function to log the job satsus in daily_log_status table.\")\n",
    "\n",
    "            # Converting the test results to write into a file\n",
    "            results_df.to_csv(f\"{self.log_path}/UnitTest.csv\", index=False)\n",
    "            logging.info(f\"Saved test results to UnitTest table.\")\n",
    "\n",
    "            # Creating a Spark session and converting the test results to a Spark DataFrame\n",
    "            spark = SparkSession.builder.appName(\"UnitTest\").getOrCreate()\n",
    "            UnitTest_df = spark.createDataFrame(results_df)\n",
    "            UnitTest_df.show(truncate=False)\n",
    "            logging.info(\"Creating a dataframe which will have daily_log_status.\")\n",
    "\n",
    "            # Creating a summary DataFrame to store load status\n",
    "            load_status_df = UnitTest_df.groupby(\"Table Name\") \\\n",
    "                .agg(F.max(\"Status\").alias(\"Status\"), F.lit(\"UnitTest\").alias(\"Test Type\"), F.current_date().alias(\"Date\"))\n",
    "\n",
    "            # Checking if any row in the test results DataFrame has \"FAIL\" in the \"Status\" column\n",
    "            if \"FAIL\" in load_status_df.select(\"Status\").distinct().rdd.map(lambda x: x[0].upper()).collect():\n",
    "                # Set the status in the summary DataFrame to \"FAIL\"\n",
    "                load_status_df = load_status_df.withColumn(\"Status\", F.lit(\"FAIL\"))\n",
    "                logging.info(\"Set load status to FAIL if any of the units test cases failed.\")\n",
    "\n",
    "            # Define the file path for the load status CSV\n",
    "            csv_file_path = \"load_status\"\n",
    "\n",
    "            # Check if the log file already exists\n",
    "            if os.path.isfile(f\"{self.log_path}/{csv_file_path}.csv\"):\n",
    "                # Reading the existing load status CSV directly\n",
    "                existing_load_status_df = spark.read.csv(f\"{self.log_path}/{csv_file_path}.csv\", header=True, inferSchema=True)\n",
    "                final_load_status_df = existing_load_status_df.union(load_status_df).distinct()\n",
    "                final_load_status_dfp = final_load_status_df.toPandas()\n",
    "                final_load_status_dfp.to_csv(f\"{self.log_path}/load_status.csv\", index=False)\n",
    "                logging.info(f\"Appended the today's load status to table load_status.\")\n",
    "            else:\n",
    "                # if the file doesn't exist, creating it with the current load status\n",
    "                final_load_status_df = load_status_df\n",
    "                final_load_status_dfp = final_load_status_df.toPandas()\n",
    "                final_load_status_dfp.to_csv(f\"{self.log_path}/load_status.csv\", mode='a', index=False, header=not os.path.exists(f\"{self.log_path}/load_status.csv\"))\n",
    "                logging.info(f\"Created {self.log_path}/load_status.csv with current load status\")\n",
    "\n",
    "            # Log the daily status end\n",
    "            logging.info(\"Completed updating the daily_log_status for today's job status.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in daily_log_status: {str(e)}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Creating an empty list to store test results\n",
    "    test_results = []\n",
    "    \n",
    "    # Creating a test suite\n",
    "    test_suite = unittest.TestLoader().loadTestsFromTestCase(TestCharStats)\n",
    "\n",
    "    # Running the tests\n",
    "    test_runner = unittest.TextTestRunner()\n",
    "\n",
    "    # Run each test and log the results\n",
    "    for test_case in test_suite:\n",
    "        test_result = test_runner.run(test_case)\n",
    "\n",
    "    # Converting the test results to write into a file\n",
    "    results_df = pd.DataFrame(test_results)\n",
    "\n",
    "    # Call the daily_log_status function with test_results as an argument\n",
    "    test_instance = TestCharStats()\n",
    "    test_instance.daily_log_status(results_df)\n",
    "    print(\"UnitTesting Job completed successfully.\")\n",
    "    logging.info(\"UnitTesting Job completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8810d8-c35e-4610-851e-3150388fc98c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
