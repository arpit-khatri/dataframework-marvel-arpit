{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1969dcfa-d804-4027-adfa-9d8c0c217519",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 1.231s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Table Name</th>\n",
       "      <th>Status</th>\n",
       "      <th>Test Type</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>char_stats_day_dly</td>\n",
       "      <td>Passed</td>\n",
       "      <td>Unit Testing</td>\n",
       "      <td>2023-09-16 18:26:21.871675</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Table Name  Status     Test Type                       Date\n",
       "0  char_stats_day_dly  Passed  Unit Testing 2023-09-16 18:26:21.871675"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "import yaml\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "import argparse\n",
    "\n",
    "# Define command-line arguments\n",
    "#parser = argparse.ArgumentParser(description=\"Unit test script for Spark DataFrame schema and data validation\")\n",
    "#parser.add_argument(\"--table_name\", required=True, help=\"Path to the target table\")\n",
    "\n",
    "class TestCharStats(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        # Parse command-line arguments\n",
    "        #args = parser.parse_args()\n",
    "        #self.table_name = args.table_name\n",
    "\n",
    "        # Loading configuration from config.yaml\n",
    "        with open(\"config.yaml\", \"r\") as config_file:\n",
    "            config = yaml.safe_load(config_file)\n",
    "\n",
    "        # Create a Spark session\n",
    "        self.spark = SparkSession.builder.appName(\"TestCharStats\").getOrCreate()\n",
    "\n",
    "        # Set the log level to ERROR or FATAL\n",
    "        self.spark.sparkContext.setLogLevel(\"ERROR\")  # You can change \"ERROR\" to \"FATAL\" if needed\n",
    "\n",
    "        # Assign configuration variables\n",
    "        self.columns_config = config[\"columns\"]\n",
    "        self.table_name = config[\"table_name\"]\n",
    "        self.output_path = config[\"output_path\"]\n",
    "        self.absolute_table_name =f\"{self.output_path}/{self.table_name}\"\n",
    "\n",
    "    def tearDown(self):\n",
    "        # Stop the Spark session\n",
    "        self.spark.stop()\n",
    "\n",
    "    def log_test_result(self, test_name, status, reason=\"\"):\n",
    "        timestamp = datetime.datetime.now()\n",
    "        # Creating a dictionary for the test result\n",
    "        result = {\n",
    "            \"Table Name\": self.table_name,\n",
    "            \"Test Name\": test_name,\n",
    "            \"Status\": status,\n",
    "            \"Reason\": reason,\n",
    "            \"Timestamp\": timestamp\n",
    "        }\n",
    "        # Append the result to the test_results list\n",
    "        test_results.append(result)\n",
    "\n",
    "    def test_column_data_types(self):\n",
    "        # Loading the current data\n",
    "        df = self.spark.read.parquet(self.absolute_table_name)\n",
    "\n",
    "        # Check if column data types match the configuration\n",
    "        for column_config in self.columns_config:\n",
    "            column_name = column_config[\"name\"]\n",
    "            expected_data_type = column_config[\"type\"]\n",
    "            actual_data_type = df.schema[column_name].dataType.simpleString()\n",
    "            if actual_data_type == expected_data_type:\n",
    "                self.log_test_result(f\"{column_name} data_type\", \"PASSED\")\n",
    "            else:\n",
    "                self.log_test_result(f\"{column_name} data_type\", \"FAILED\", f\"Expected {expected_data_type}, but got {actual_data_type}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Creating an empty list to store test results\n",
    "    test_results = []\n",
    "\n",
    "    # Creating a test suite\n",
    "    test_suite = unittest.TestLoader().loadTestsFromTestCase(TestCharStats)\n",
    "\n",
    "    # Running the tests\n",
    "    test_runner = unittest.TextTestRunner()\n",
    "\n",
    "    # Run each test and log the results\n",
    "    for test_case in test_suite:\n",
    "        test_result = test_runner.run(test_case)\n",
    "\n",
    "    # Converting the test results to write into a file\n",
    "    results_df = pd.DataFrame(test_results)\n",
    "\n",
    "#print(results_df.head())\n",
    "# Create a summary DataFrame to store load status\n",
    "load_status_df = results_df.groupby([\"Table Name\"])[\"Status\"].max().reset_index()\n",
    "load_status_df.loc[:, \"Test Type\"] = \"Unit Testing\"\n",
    "\n",
    "# Check if any row in the test results DataFrame has \"Failed\" in the \"Status\" column\n",
    "if \"FAILED\" in load_status_df[\"Status\"].str.upper().values:\n",
    "    # Set the status in the summary DataFrame to \"Failed\"\n",
    "    load_status_df.loc[:, \"Status\"] = \"Failed\"\n",
    "else:\n",
    "    load_status_df.loc[:, \"Status\"] = \"Passed\"\n",
    "\n",
    "# Add a Date column with the current date and time\n",
    "load_status_df[\"Date\"] = pd.to_datetime('today')\n",
    "\n",
    "# Display the summary load status\n",
    "load_status_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b450efc2-01ef-4b77-820d-00dfb41fdacb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
