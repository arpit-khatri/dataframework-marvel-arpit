{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9aef0ec-faae-439b-a5ec-c83b45d636a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_count_increase (__main__.TestCharStats.test_count_increase) ... ok\n",
      "test_primary_key (__main__.TestCharStats.test_primary_key) ... FAIL\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_primary_key (__main__.TestCharStats.test_primary_key)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_20551/1710263554.py\", line 33, in test_primary_key\n",
      "    self.assertEqual(num_rows, num_distinct_rows)\n",
      "AssertionError: 198 != 197\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 2.641s\n",
      "\n",
      "FAILED (failures=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=2 errors=0 failures=1>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "import yaml\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "class TestCharStats(unittest.TestCase):\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        # Load configuration from config.yaml\n",
    "        with open(\"config.yaml\", \"r\") as config_file:\n",
    "            config = yaml.safe_load(config_file)\n",
    "\n",
    "        # Create a Spark session\n",
    "        cls.spark = SparkSession.builder.appName(\"TestCharStats\").getOrCreate()\n",
    "        \n",
    "        # Assign configuration variables\n",
    "        cls.table_name = config[\"table_name\"]\n",
    "        cls.primary_key_columns = config[\"primary_key_columns\"]\n",
    "        cls.threshold_percentage = config[\"threshold_percentage\"]\n",
    "\n",
    "    @classmethod\n",
    "    def tearDownClass(cls):\n",
    "        # Stop the Spark session\n",
    "        cls.spark.stop()\n",
    "\n",
    "    def test_primary_key(self):\n",
    "        # Load your DataFrame\n",
    "        df = self.spark.read.parquet(\"/home/jovyan/work/digital/target/char_stats/\")  # Replace with the actual path\n",
    "\n",
    "        # Check if the primary key columns have no duplicates\n",
    "        num_rows = df.count()\n",
    "        num_distinct_rows = df.select(*self.primary_key_columns).distinct().count()\n",
    "        self.assertEqual(num_rows, num_distinct_rows)\n",
    "\n",
    "    def test_count_increase(self):\n",
    "        # Load your DataFrames for previous and current loads\n",
    "        df_previous = self.spark.read.parquet(\"/home/jovyan/work/digital/target/char_stats/\")\n",
    "        df_current = self.spark.read.parquet(\"/home/jovyan/work/digital/target/char_stats/\")  # Replace with the actual path\n",
    "\n",
    "        # Calculate the count increase percentage\n",
    "        count_previous = df_previous.count()\n",
    "        count_current = df_current.count()\n",
    "        increase_percentage = (count_current - count_previous) / count_previous * 100\n",
    "\n",
    "        # Check if the count increase is not less than the threshold percentage\n",
    "        self.assertLessEqual(increase_percentage, self.threshold_percentage)\n",
    "\n",
    "# Load and run the tests from Jupyter Notebook\n",
    "test_suite = unittest.TestLoader().loadTestsFromTestCase(TestCharStats)\n",
    "unittest.TextTestRunner(verbosity=2).run(test_suite)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c80e12d1-3061-4114-ab18-4bc05ff1c3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".F\n",
      "======================================================================\n",
      "FAIL: test_primary_key (__main__.TestCharStats.test_primary_key)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_20551/1729191441.py\", line 37, in test_primary_key\n",
      "    self.assertEqual(num_rows, num_distinct_rows)\n",
      "AssertionError: 198 != 197\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 2.982s\n",
      "\n",
      "FAILED (failures=1)\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "import yaml\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Create a list to store test results\n",
    "test_results = []\n",
    "\n",
    "class TestCharStats(unittest.TestCase):\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        # Load configuration from config.yaml\n",
    "        with open(\"config.yaml\", \"r\") as config_file:\n",
    "            config = yaml.safe_load(config_file)\n",
    "\n",
    "        # Create a Spark session\n",
    "        cls.spark = SparkSession.builder.appName(\"TestCharStats\").getOrCreate()\n",
    "        \n",
    "        # Assign configuration variables\n",
    "        cls.table_name = config[\"table_name\"]\n",
    "        cls.primary_key_columns = config[\"primary_key_columns\"]\n",
    "        cls.threshold_percentage = config[\"threshold_percentage\"]\n",
    "\n",
    "    @classmethod\n",
    "    def tearDownClass(cls):\n",
    "        # Stop the Spark session\n",
    "        cls.spark.stop()\n",
    "\n",
    "    def test_primary_key(self):\n",
    "        # Load your DataFrame\n",
    "        df = self.spark.read.parquet(\"/home/jovyan/work/digital/target/char_stats/\")  # Replace with the actual path\n",
    "\n",
    "        # Check if the primary key columns have no duplicates\n",
    "        num_rows = df.count()\n",
    "        num_distinct_rows = df.select(*self.primary_key_columns).distinct().count()\n",
    "        self.assertEqual(num_rows, num_distinct_rows)\n",
    "\n",
    "    def test_count_increase(self):\n",
    "        # Load your DataFrames for previous and current loads\n",
    "        df_previous = self.spark.read.parquet(\"/home/jovyan/work/digital/target/char_stats/\")\n",
    "        df_current = self.spark.read.parquet(\"/home/jovyan/work/digital/target/char_stats/\")  # Replace with the actual path\n",
    "\n",
    "        # Calculate the count increase percentage\n",
    "        count_previous = df_previous.count()\n",
    "        count_current = df_current.count()\n",
    "        increase_percentage = (count_current - count_previous) / count_previous * 100\n",
    "\n",
    "        # Check if the count increase is not less than the threshold percentage\n",
    "        self.assertLessEqual(increase_percentage, self.threshold_percentage)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Create a Spark session\n",
    "    spark = SparkSession.builder.appName(\"TestCharStats\").getOrCreate()\n",
    "\n",
    "    # Load configuration from config.yaml\n",
    "    with open(\"config.yaml\", \"r\") as config_file:\n",
    "        config = yaml.safe_load(config_file)\n",
    "\n",
    "    # Assign configuration variables\n",
    "    table_name = config[\"table_name\"]\n",
    "    primary_key_columns = config[\"primary_key_columns\"]\n",
    "    threshold_percentage = config[\"threshold_percentage\"]\n",
    "\n",
    "    # Create a test suite\n",
    "    test_suite = unittest.TestLoader().loadTestsFromTestCase(TestCharStats)\n",
    "\n",
    "    # Run the tests\n",
    "    test_runner = unittest.TextTestRunner()\n",
    "    test_result = test_runner.run(test_suite)\n",
    "\n",
    "    # Log test results\n",
    "    for test_case, error in test_result.failures:\n",
    "        test_results.append({\n",
    "            \"Test Case\": test_case.id(),\n",
    "            \"Status\": \"Fail\",\n",
    "            \"Timestamp\": datetime.now()\n",
    "        })\n",
    "\n",
    "    for test_case, error in test_result.errors:\n",
    "        test_results.append({\n",
    "            \"Test Case\": test_case.id(),\n",
    "            \"Status\": \"Error\",\n",
    "            \"Timestamp\": datetime.now()\n",
    "        })\n",
    "\n",
    "    # Collect successful test cases\n",
    "    all_test_cases = test_suite.countTestCases()\n",
    "    failed_test_cases = len(test_result.failures) + len(test_result.errors)\n",
    "    successful_test_cases = all_test_cases - failed_test_cases\n",
    "\n",
    "    for i in range(successful_test_cases):\n",
    "        test_results.append({\n",
    "            \"Test Case\": f\"Test {i+1}\",\n",
    "            \"Status\": \"Pass\",\n",
    "            \"Timestamp\": datetime.now()\n",
    "        })\n",
    "\n",
    "    # Convert the test results to a DataFrame\n",
    "    test_results_df = pd.DataFrame(test_results)\n",
    "\n",
    "    # Write the DataFrame to a CSV file\n",
    "    test_results_df.to_csv(\"test_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78226c88-0329-47cb-a216-07b1127ec585",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
