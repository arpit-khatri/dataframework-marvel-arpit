{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1969dcfa-d804-4027-adfa-9d8c0c217519",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 1.322s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Table Name</th>\n",
       "      <th>Status</th>\n",
       "      <th>Test Type</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>char_stats_day_dly</td>\n",
       "      <td>PASS</td>\n",
       "      <td>Unit Testing</td>\n",
       "      <td>2023-09-16 19:56:42.732584</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Table Name Status     Test Type                       Date\n",
       "0  char_stats_day_dly   PASS  Unit Testing 2023-09-16 19:56:42.732584"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "import yaml\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "import argparse\n",
    "\n",
    "# Define command-line arguments\n",
    "#parser = argparse.ArgumentParser(description=\"Unit test script for Spark DataFrame schema and data validation\")\n",
    "#parser.add_argument(\"--table_name\", required=True, help=\"Path to the target table\")\n",
    "\n",
    "class TestCharStats(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        # Parse command-line arguments\n",
    "        #args = parser.parse_args()\n",
    "        #self.table_name = args.table_name\n",
    "\n",
    "        # Loading configuration from config.yaml\n",
    "        with open(\"config.yaml\", \"r\") as config_file:\n",
    "            config = yaml.safe_load(config_file)\n",
    "\n",
    "        # Create a Spark session\n",
    "        self.spark = SparkSession.builder.appName(\"TestCharStats\").getOrCreate()\n",
    "\n",
    "        # Set the log level to ERROR or FATAL\n",
    "        self.spark.sparkContext.setLogLevel(\"ERROR\")  # You can change \"ERROR\" to \"FATAL\" if needed\n",
    "\n",
    "        # Assign configuration variables\n",
    "        self.columns_config = config[\"columns\"]\n",
    "        self.table_name = config[\"table_name\"]\n",
    "        self.output_path = config[\"output_path\"]\n",
    "        self.absolute_table_name =f\"{self.output_path}/{self.table_name}\"\n",
    "\n",
    "    def tearDown(self):\n",
    "        # Stop the Spark session\n",
    "        self.spark.stop()\n",
    "\n",
    "    def log_test_result(self, test_name, status, reason=\"\"):\n",
    "        timestamp = datetime.datetime.now()\n",
    "        # Creating a dictionary for the test result\n",
    "        result = {\n",
    "            \"Table Name\": self.table_name,\n",
    "            \"Test Name\": test_name,\n",
    "            \"Status\": status,\n",
    "            \"Reason\": reason,\n",
    "            \"Timestamp\": timestamp\n",
    "        }\n",
    "        # Append the result to the test_results list\n",
    "        test_results.append(result)\n",
    "\n",
    "    def test_column_data_types(self):\n",
    "        # Loading the current data\n",
    "        df = self.spark.read.parquet(self.absolute_table_name)\n",
    "\n",
    "        # Check if column data types match the configuration\n",
    "        for column_config in self.columns_config:\n",
    "            column_name = column_config[\"name\"]\n",
    "            expected_data_type = column_config[\"type\"]\n",
    "            actual_data_type = df.schema[column_name].dataType.simpleString()\n",
    "            if actual_data_type == expected_data_type:\n",
    "                self.log_test_result(f\"{column_name} data_type\", \"PASS\")\n",
    "            else:\n",
    "                self.log_test_result(f\"{column_name} data_type\", \"FAIL\", f\"Expected {expected_data_type}, but got {actual_data_type}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Creating an empty list to store test results\n",
    "    test_results = []\n",
    "\n",
    "    # Creating a test suite\n",
    "    test_suite = unittest.TestLoader().loadTestsFromTestCase(TestCharStats)\n",
    "\n",
    "    # Running the tests\n",
    "    test_runner = unittest.TextTestRunner()\n",
    "\n",
    "    # Run each test and log the results\n",
    "    for test_case in test_suite:\n",
    "        test_result = test_runner.run(test_case)\n",
    "\n",
    "    # Converting the test results to write into a file\n",
    "    results_df = pd.DataFrame(test_results)\n",
    "\n",
    "#print(results_df.head())\n",
    "# Create a summary DataFrame to store load status\n",
    "load_status_df = results_df.groupby([\"Table Name\"])[\"Status\"].max().reset_index()\n",
    "load_status_df.loc[:, \"Test Type\"] = \"Unit Testing\"\n",
    "\n",
    "# Check if any row in the test results DataFrame has \"Failed\" in the \"Status\" column\n",
    "if \"FAIL\" in load_status_df[\"Status\"].str.upper().values:\n",
    "    # Set the status in the summary DataFrame to \"Failed\"\n",
    "    load_status_df.loc[:, \"Status\"] = \"FAIL\"\n",
    "else:\n",
    "    load_status_df.loc[:, \"Status\"] = \"PASS\"\n",
    "\n",
    "# Add a Date column with the current date and time\n",
    "load_status_df[\"Date\"] = pd.to_datetime('today')\n",
    "\n",
    "# Display the summary load status\n",
    "load_status_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b450efc2-01ef-4b77-820d-00dfb41fdacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 1.355s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------+-----------+----------+\n",
      "|        Table Name|Status|  Test Type|      Date|\n",
      "+------------------+------+-----------+----------+\n",
      "|char_stats_day_dly|  PASS|UnitTesting|2023-09-16|\n",
      "+------------------+------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "import yaml\n",
    "import datetime\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import col, upper\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "\n",
    "# Define command-line arguments\n",
    "#parser = argparse.ArgumentParser(description=\"Unit test script for Spark DataFrame schema and data validation\")\n",
    "#parser.add_argument(\"--table_name\", required=True, help=\"Path to the target table\")\n",
    "\n",
    "class TestCharStats(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        # Parse command-line arguments\n",
    "        #args = parser.parse_args()\n",
    "        #self.table_name = args.table_name\n",
    "\n",
    "        # Loading configuration from config.yaml\n",
    "        with open(\"config.yaml\", \"r\") as config_file:\n",
    "            config = yaml.safe_load(config_file)\n",
    "\n",
    "        # Create a Spark session\n",
    "        self.spark = SparkSession.builder.appName(\"TestCharStats\").getOrCreate()\n",
    "\n",
    "        # Set the log level to ERROR or FATAL\n",
    "        self.spark.sparkContext.setLogLevel(\"ERROR\")  # You can change \"ERROR\" to \"FATAL\" if needed\n",
    "\n",
    "        # Assign configuration variables\n",
    "        self.columns_config = config[\"columns\"]\n",
    "        self.table_name = config[\"table_name\"]\n",
    "        self.output_path = config[\"output_path\"]\n",
    "        self.absolute_table_name =f\"{self.output_path}/{self.table_name}\"\n",
    "\n",
    "    def tearDown(self):\n",
    "        # Stop the Spark session\n",
    "        self.spark.stop()\n",
    "\n",
    "    def log_test_result(self, test_name, status, reason=\"\"):\n",
    "        timestamp = datetime.datetime.now()\n",
    "        # Creating a dictionary for the test result\n",
    "        result = {\n",
    "            \"Table Name\": self.table_name,\n",
    "            \"Test Name\": test_name,\n",
    "            \"Status\": status,\n",
    "            \"Reason\": reason,\n",
    "            \"Timestamp\": timestamp\n",
    "        }\n",
    "        # Append the result to the test_results list\n",
    "        test_results.append(result)\n",
    "\n",
    "    def test_column_data_types(self):\n",
    "        # Loading the current data\n",
    "        df = self.spark.read.parquet(self.absolute_table_name)\n",
    "\n",
    "        # Check if column data types match the configuration\n",
    "        for column_config in self.columns_config:\n",
    "            column_name = column_config[\"name\"]\n",
    "            expected_data_type = column_config[\"type\"]\n",
    "            actual_data_type = df.schema[column_name].dataType.simpleString()\n",
    "            if actual_data_type == expected_data_type:\n",
    "                self.log_test_result(f\"{column_name} data_type\", \"PASS\")\n",
    "            else:\n",
    "                self.log_test_result(f\"{column_name} data_type\", \"FAIL\", f\"Expected {expected_data_type}, but got {actual_data_type}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Creating an empty list to store test results\n",
    "    test_results = []\n",
    "\n",
    "    # Creating a test suite\n",
    "    test_suite = unittest.TestLoader().loadTestsFromTestCase(TestCharStats)\n",
    "\n",
    "    # Running the tests\n",
    "    test_runner = unittest.TextTestRunner()\n",
    "\n",
    "    # Run each test and log the results\n",
    "    for test_case in test_suite:\n",
    "        test_result = test_runner.run(test_case)\n",
    "\n",
    "    # Converting the test results to write into a file\n",
    "    results_df = pd.DataFrame(test_results)\n",
    "    results_df.to_csv(\"UnitTesting.csv\", index=False)\n",
    "\n",
    "    # Create a Spark session and converting the test results to spark dataframe\n",
    "    spark = SparkSession.builder.appName(\"UnitTesting\").getOrCreate()\n",
    "    #schema = [\"Table Name\", \"Status\", \"Test Type\", \"Date\"]\n",
    "\n",
    "    UnitTest_df = spark.createDataFrame(results_df)\n",
    "    \n",
    "    \n",
    "     # Create a summary DataFrame to store load status\n",
    "    load_status_df = UnitTest_df.groupby(\"Table Name\")\\\n",
    "        .agg(F.max(\"Status\").alias(\"Status\"), F.lit(\"UnitTesting\").alias(\"Test Type\"), F.current_date().alias(\"Date\"))\n",
    "    # Check if any row in the test results DataFrame has \"FAIL\" in the \"Status\" column\n",
    "    if \"FAIL\" in load_status_df.select(\"Status\").distinct().rdd.map(lambda x: x[0].upper()).collect():\n",
    "        # Set the status in the summary DataFrame to \"FAIL\"\n",
    "        load_status_df = load_status_df.withColumn(\"Status\", F.lit(\"FAIL\"))\n",
    "    # Define the file path for the load status CSV\n",
    "    csv_file_path = \"load_status\"\n",
    "    \n",
    "    # Check if the file already exists\n",
    "    #if os.path.isfile(f\"work/digital/logs/{csv_file_path}.csv\"):\n",
    "    if os.path.isfile(f\"{csv_file_path}.csv\"):\n",
    "        # Read the existing load status CSV directly\n",
    "        existing_load_status_df = spark.read.csv(f\"{csv_file_path}.csv\", header=True, inferSchema=True)\n",
    "        # Check if the date matches the current date\n",
    "        current_date = spark.sql(\"SELECT current_date() as current_date\").collect()[0][\"current_date\"]\n",
    "        matching_date_rows = existing_load_status_df.filter(existing_load_status_df[\"Date\"] == current_date)\n",
    "        if not matching_date_rows.isEmpty():\n",
    "            # Overwrite the existing values for the current date\n",
    "            existing_load_status_df = existing_load_status_df.filter(existing_load_status_df[\"Date\"] != current_date)\n",
    "            existing_load_status_df = existing_load_status_df.union(load_status_df)\n",
    "    else:\n",
    "        # The file doesn't exist, create it with the current load status\n",
    "        existing_load_status_df = load_status_df\n",
    "    # Write the updated or new load status to the CSV file\n",
    "    existing_load_status_df.show()\n",
    "    existing_load_status_dfp=existing_load_status_df.toPandas()\n",
    "    existing_load_status_dfp.to_csv(\"load_status.csv\", mode='a', index=False, header=not os.path.exists(\"load_status.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41e7caa-49e5-48f1-86d8-ec6c02e9462a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
