{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b450efc2-01ef-4b77-820d-00dfb41fdacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.636s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------------------+------+------+--------------------------+\n",
      "|Table Name        |Test Name             |Status|Reason|Timestamp                 |\n",
      "+------------------+----------------------+------+------+--------------------------+\n",
      "|char_stats_day_dly|name data_type        |PASS  |      |2023-09-17 06:08:56.13323 |\n",
      "|char_stats_day_dly|characterID data_type |PASS  |      |2023-09-17 06:08:56.133243|\n",
      "|char_stats_day_dly|Alignment data_type   |PASS  |      |2023-09-17 06:08:56.133247|\n",
      "|char_stats_day_dly|Intelligence data_type|PASS  |      |2023-09-17 06:08:56.13325 |\n",
      "|char_stats_day_dly|Strength data_type    |PASS  |      |2023-09-17 06:08:56.133253|\n",
      "|char_stats_day_dly|Speed data_type       |PASS  |      |2023-09-17 06:08:56.133256|\n",
      "|char_stats_day_dly|Durability data_type  |PASS  |      |2023-09-17 06:08:56.133259|\n",
      "|char_stats_day_dly|Power data_type       |PASS  |      |2023-09-17 06:08:56.133268|\n",
      "|char_stats_day_dly|Combat data_type      |PASS  |      |2023-09-17 06:08:56.133272|\n",
      "|char_stats_day_dly|Total data_type       |PASS  |      |2023-09-17 06:08:56.133276|\n",
      "|char_stats_day_dly|batch_id data_type    |PASS  |      |2023-09-17 06:08:56.13328 |\n",
      "|char_stats_day_dly|load_date data_type   |PASS  |      |2023-09-17 06:08:56.133284|\n",
      "+------------------+----------------------+------+------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "import yaml\n",
    "import datetime\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import col, upper\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "\n",
    "# Define command-line arguments\n",
    "#parser = argparse.ArgumentParser(description=\"Unit test script for Spark DataFrame schema and data validation\")\n",
    "#parser.add_argument(\"--table_name\", required=True, help=\"Path to the target table\")\n",
    "\n",
    "class TestCharStats(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        # Parse command-line arguments\n",
    "        #args = parser.parse_args()\n",
    "        #self.table_name = args.table_name\n",
    "\n",
    "        # Loading configuration from config.yaml\n",
    "        with open(\"config.yaml\", \"r\") as config_file:\n",
    "            config = yaml.safe_load(config_file)\n",
    "\n",
    "        # Create a Spark session\n",
    "        self.spark = SparkSession.builder.appName(\"TestCharStats\").getOrCreate()\n",
    "\n",
    "        # Set the log level to ERROR or FATAL\n",
    "        self.spark.sparkContext.setLogLevel(\"ERROR\")  # You can change \"ERROR\" to \"FATAL\" if needed\n",
    "\n",
    "        # Assign configuration variables\n",
    "        self.columns_config = config[\"columns\"]\n",
    "        self.table_name = config[\"table_name\"]\n",
    "        self.output_path = config[\"output_path\"]\n",
    "        self.log_path = config[\"monitoring_path\"]\n",
    "        self.absolute_table_name =f\"{self.output_path}/{self.table_name}\"\n",
    "\n",
    "    def tearDown(self):\n",
    "        # Stop the Spark session\n",
    "        self.spark.stop()\n",
    "\n",
    "    def log_test_result(self, test_name, status, reason=\"\"):\n",
    "        timestamp = datetime.datetime.now()\n",
    "        # Creating a dictionary for the test result\n",
    "        result = {\n",
    "            \"Table Name\": self.table_name,\n",
    "            \"Test Name\": test_name,\n",
    "            \"Status\": status,\n",
    "            \"Reason\": reason,\n",
    "            \"Timestamp\": timestamp\n",
    "        }\n",
    "        # Append the result to the test_results list\n",
    "        test_results.append(result)\n",
    "\n",
    "    def test_column_data_types(self):\n",
    "        # Loading the current data\n",
    "        df = self.spark.read.parquet(self.absolute_table_name)\n",
    "\n",
    "        # Check if column data types match the configuration\n",
    "        for column_config in self.columns_config:\n",
    "            column_name = column_config[\"name\"]\n",
    "            expected_data_type = column_config[\"type\"]\n",
    "            actual_data_type = df.schema[column_name].dataType.simpleString()\n",
    "            if actual_data_type == expected_data_type:\n",
    "                self.log_test_result(f\"{column_name} data_type\", \"PASS\")\n",
    "            else:\n",
    "                self.log_test_result(f\"{column_name} data_type\", \"FAIL\", f\"Expected {expected_data_type}, but got {actual_data_type}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Creating an empty list to store test results\n",
    "    test_results = []\n",
    "\n",
    "    # Creating a test suite\n",
    "    test_suite = unittest.TestLoader().loadTestsFromTestCase(TestCharStats)\n",
    "\n",
    "    # Running the tests\n",
    "    test_runner = unittest.TextTestRunner()\n",
    "\n",
    "    # Run each test and log the results\n",
    "    for test_case in test_suite:\n",
    "        test_result = test_runner.run(test_case)\n",
    "\n",
    "    # Converting the test results to write into a file\n",
    "    results_df = pd.DataFrame(test_results)\n",
    "    results_df.to_csv(\"UnitTesting.csv\", index=False)\n",
    "\n",
    "    # Create a Spark session and converting the test results to spark dataframe\n",
    "    spark = SparkSession.builder.appName(\"UnitTesting\").getOrCreate()\n",
    "    #schema = [\"Table Name\", \"Status\", \"Test Type\", \"Date\"]\n",
    "\n",
    "    UnitTest_df = spark.createDataFrame(results_df)\n",
    "    UnitTest_df.show(truncate=False)\n",
    "\n",
    "     # Create a summary DataFrame to store load status\n",
    "    load_status_df = UnitTest_df.groupby(\"Table Name\")\\\n",
    "        .agg(F.max(\"Status\").alias(\"Status\"), F.lit(\"UnitTesting\").alias(\"Test Type\"), F.current_date().alias(\"Date\"))\n",
    "    # Check if any row in the test results DataFrame has \"FAIL\" in the \"Status\" column\n",
    "    if \"FAIL\" in load_status_df.select(\"Status\").distinct().rdd.map(lambda x: x[0].upper()).collect():\n",
    "        # Set the status in the summary DataFrame to \"FAIL\"\n",
    "        load_status_df = load_status_df.withColumn(\"Status\", F.lit(\"FAIL\"))\n",
    "    \n",
    "    # Define the file path for the load status CSV\n",
    "    csv_file_path = \"load_status\"\n",
    "    # Check if the file already exists\n",
    "    #if os.path.isfile(f\"work/digital/logs/{csv_file_path}.csv\"):\n",
    "    if os.path.isfile(f\"{csv_file_path}.csv\"):\n",
    "        ## Read the existing load status CSV directly\n",
    "        existing_load_status_df = spark.read.csv(f\"{csv_file_path}.csv\", header=True, inferSchema=True)\n",
    "        final_load_status_df = existing_load_status_df.union(load_status_df).distinct()\n",
    "        final_load_status_dfp=final_load_status_df.toPandas()\n",
    "        final_load_status_dfp.to_csv(\"load_status.csv\",index=False)\n",
    "    else:\n",
    "        # The file doesn't exist, create it with the current load status\n",
    "        final_load_status_df = load_status_df\n",
    "        final_load_status_dfp=final_load_status_df.toPandas()\n",
    "        final_load_status_dfp.to_csv(\"load_status.csv\", mode='a', index=False, header=not os.path.exists(\"load_status.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e41e7caa-49e5-48f1-86d8-ec6c02e9462a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E\n",
      "======================================================================\n",
      "ERROR: test_column_data_types (__main__.TestCharStats.test_column_data_types)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_29810/743677907.py\", line 55, in test_column_data_types\n",
      "    df = self.spark.read.parquet(self.absolute_table_name)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/sql/readwriter.py\", line 531, in parquet\n",
      "    return self._df(self._jreader.parquet(_to_seq(self._spark._sc, paths)))\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/errors/exceptions/captured.py\", line 175, in deco\n",
      "    raise converted from None\n",
      "pyspark.errors.exceptions.captured.AnalysisException: [PATH_NOT_FOUND] Path does not exist: file:/home/jovyan/work/digital/monitoring/char_stats_day_dly.\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.390s\n",
      "\n",
      "FAILED (errors=1)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 120\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m#results_df.to_csv(\"UnitTesting.csv\", index=False)\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# Call the daily_log_status function with test_results as an argument\u001b[39;00m\n\u001b[1;32m    119\u001b[0m test_instance \u001b[38;5;241m=\u001b[39m TestCharStats()\n\u001b[0;32m--> 120\u001b[0m \u001b[43mtest_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdaily_log_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJob completed successfully\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 72\u001b[0m, in \u001b[0;36mTestCharStats.daily_log_status\u001b[0;34m(self, results_df)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Creating a Spark session and converting the test results to spark dataframe\u001b[39;00m\n\u001b[1;32m     71\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnitTesting\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[0;32m---> 72\u001b[0m UnitTest_df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m UnitTest_df\u001b[38;5;241m.\u001b[39mshow(truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Creating a summary DataFrame to store load status\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:1273\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1269\u001b[0m     data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data, columns\u001b[38;5;241m=\u001b[39mcolumn_names)\n\u001b[1;32m   1271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m   1272\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[0;32m-> 1273\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSparkSession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\n\u001b[1;32m   1275\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1276\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dataframe(\n\u001b[1;32m   1277\u001b[0m     data, schema, samplingRatio, verifySchema  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   1278\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/pandas/conversion.py:439\u001b[0m, in \u001b[0;36mSparkConversionMixin.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    437\u001b[0m             warn(msg)\n\u001b[1;32m    438\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m--> 439\u001b[0m converted_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_from_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimezone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dataframe(converted_data, schema, samplingRatio, verifySchema)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/pandas/conversion.py:503\u001b[0m, in \u001b[0;36mSparkConversionMixin._convert_from_pandas\u001b[0;34m(self, pdf, schema, timezone)\u001b[0m\n\u001b[1;32m    498\u001b[0m             pdf[column] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(\n\u001b[1;32m    499\u001b[0m                 ser\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mto_pytimedelta(), index\u001b[38;5;241m=\u001b[39mser\u001b[38;5;241m.\u001b[39mindex, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39mser\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m    500\u001b[0m             )\n\u001b[1;32m    502\u001b[0m \u001b[38;5;66;03m# Convert pandas.DataFrame to list of numpy records\u001b[39;00m\n\u001b[0;32m--> 503\u001b[0m np_records \u001b[38;5;241m=\u001b[39m \u001b[43mpdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_records\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;66;03m# Check if any columns need to be fixed for Spark to infer properly\u001b[39;00m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(np_records) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:2551\u001b[0m, in \u001b[0;36mDataFrame.to_records\u001b[0;34m(self, index, column_dtypes, index_dtypes)\u001b[0m\n\u001b[1;32m   2548\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid dtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype_mapping\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m specified for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00melement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2549\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromarrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnames\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mformats\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mformats\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/numpy/core/records.py:643\u001b[0m, in \u001b[0;36mfromarrays\u001b[0;34m(arrayList, dtype, shape, formats, names, titles, aligned, byteorder)\u001b[0m\n\u001b[1;32m    640\u001b[0m shape \u001b[38;5;241m=\u001b[39m _deprecate_shape_0_as_None(shape)\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 643\u001b[0m     shape \u001b[38;5;241m=\u001b[39m \u001b[43marrayList\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    644\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(shape, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    645\u001b[0m     shape \u001b[38;5;241m=\u001b[39m (shape,)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "import yaml\n",
    "import datetime\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import col, upper\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "\n",
    "# Define command-line arguments\n",
    "#parser = argparse.ArgumentParser(description=\"Unit test script for Spark DataFrame schema and data validation\")\n",
    "#parser.add_argument(\"--table_name\", required=True, help=\"Path to the target table\")\n",
    "\n",
    "class TestCharStats(unittest.TestCase):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Loading configuration from config.yaml\n",
    "        with open(\"config.yaml\", \"r\") as config_file:\n",
    "            config = yaml.safe_load(config_file)\n",
    "\n",
    "        # Create a Spark session\n",
    "        self.spark = SparkSession.builder.appName(\"TestCharStats\").getOrCreate()\n",
    "\n",
    "        # Set the log level to ERROR or FATAL\n",
    "        self.spark.sparkContext.setLogLevel(\"ERROR\")  # You can change \"ERROR\" to \"FATAL\" if needed\n",
    "\n",
    "        # Assign configuration variables\n",
    "        self.columns_config = config[\"columns\"]\n",
    "        self.table_name = config[\"table_name\"]\n",
    "        self.output_path = config[\"output_path\"]\n",
    "        self.log_path = config[\"monitoring_path\"]\n",
    "        self.absolute_table_name = f\"{self.output_path}/{self.table_name}\"\n",
    "        \n",
    "\n",
    "    def log_test_result(self, test_name, status, reason=\"\"):\n",
    "        timestamp = datetime.datetime.now()\n",
    "        # Creating a dictionary for the test result\n",
    "        result = {\n",
    "            \"Table Name\": self.table_name,\n",
    "            \"Test Name\": test_name,\n",
    "            \"Status\": status,\n",
    "            \"Reason\": reason,\n",
    "            \"Timestamp\": timestamp\n",
    "        }\n",
    "        # Append the result to the test_results list\n",
    "        test_results.append(result)\n",
    "\n",
    "    def test_column_data_types(self):\n",
    "        # Loading the current data\n",
    "        df = self.spark.read.parquet(self.absolute_table_name)\n",
    "\n",
    "        # Check if column data types match the configuration\n",
    "        for column_config in self.columns_config:\n",
    "            column_name = column_config[\"name\"]\n",
    "            expected_data_type = column_config[\"type\"]\n",
    "            actual_data_type = df.schema[column_name].dataType.simpleString()\n",
    "            if actual_data_type == expected_data_type:\n",
    "                self.log_test_result(f\"{column_name} data_type\", \"PASS\")\n",
    "            else:\n",
    "                self.log_test_result(f\"{column_name} data_type\", \"FAIL\", f\"Expected {expected_data_type}, but got {actual_data_type}\")\n",
    "    def daily_log_status(self, results_df):\n",
    "        # Converting the test results to write into a file\n",
    "        results_df.to_csv(f\"{self.log_path}/UnitTesting.csv\", index=False)\n",
    "\n",
    "        # Creating a Spark session and converting the test results to spark dataframe\n",
    "        spark = SparkSession.builder.appName(\"UnitTesting\").getOrCreate()\n",
    "        UnitTest_df = spark.createDataFrame(results_df)\n",
    "        UnitTest_df.show(truncate=False)\n",
    "\n",
    "        # Creating a summary DataFrame to store load status\n",
    "        load_status_df = UnitTest_df.groupby(\"Table Name\") \\\n",
    "            .agg(F.max(\"Status\").alias(\"Status\"), F.lit(\"UnitTesting\").alias(\"Test Type\"), F.current_date().alias(\"Date\"))\n",
    "\n",
    "        # Checking if any row in the test results DataFrame has \"FAIL\" in the \"Status\" column\n",
    "        if \"FAIL\" in load_status_df.select(\"Status\").distinct().rdd.map(lambda x: x[0].upper()).collect():\n",
    "            # Set the status in the summary DataFrame to \"FAIL\"\n",
    "            load_status_df = load_status_df.withColumn(\"Status\", F.lit(\"FAIL\"))\n",
    "\n",
    "        # Define the file path for the load status CSV\n",
    "        csv_file_path = \"load_status\"\n",
    "\n",
    "        # Check if the log file already exists\n",
    "        if os.path.isfile(f\"{self.log_path}/{csv_file_path}.csv\"):\n",
    "            # Reading the existing load status CSV directly\n",
    "            existing_load_status_df = spark.read.csv(f\"{self.log_path}/{csv_file_path}.csv\", header=True, inferSchema=True)\n",
    "            final_load_status_df = existing_load_status_df.union(load_status_df).distinct()\n",
    "            final_load_status_dfp = final_load_status_df.toPandas()\n",
    "            final_load_status_dfp.to_csv(f\"{self.log_path}/load_status.csv\", index=False)\n",
    "        else:\n",
    "            # if the file doesn't exist, creating it with the current load status\n",
    "            final_load_status_df = load_status_df\n",
    "            final_load_status_dfp = final_load_status_df.toPandas()\n",
    "            final_load_status_dfp.to_csv(f\"{self.log_path}/load_status.csv\", mode='a', index=False, header=not os.path.exists(f\"{self.log_path}/load_status.csv\"))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Creating an empty list to store test results\n",
    "    test_results = []\n",
    "\n",
    "    # Creating a test suite\n",
    "    test_suite = unittest.TestLoader().loadTestsFromTestCase(TestCharStats)\n",
    "\n",
    "    # Running the tests\n",
    "    test_runner = unittest.TextTestRunner()\n",
    "\n",
    "    # Run each test and log the results\n",
    "    for test_case in test_suite:\n",
    "        test_result = test_runner.run(test_case)\n",
    "\n",
    "    # Converting the test results to write into a file\n",
    "    results_df = pd.DataFrame(test_results)\n",
    "    #results_df.to_csv(\"UnitTesting.csv\", index=False)\n",
    "\n",
    "    # Call the daily_log_status function with test_results as an argument\n",
    "    test_instance = TestCharStats()\n",
    "    test_instance.daily_log_status(results_df)\n",
    "    print(\"Job completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b155e57b-762e-4624-852d-6bcaa5c608c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
