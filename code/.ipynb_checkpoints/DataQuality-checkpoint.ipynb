{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78226c88-0329-47cb-a216-07b1127ec585",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E\n",
      "======================================================================\n",
      "ERROR: test_count_increase (__main__.TestCharStats.test_count_increase)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_5672/3399858392.py\", line 57, in test_count_increase\n",
      "    df_previous = cls.spark.read.parquet(\"/home/jovyan/work/digital/target/char_stats/\")\n",
      "                  ^^^\n",
      "NameError: name 'cls' is not defined\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.003s\n",
      "\n",
      "FAILED (errors=1)\n",
      "E\n",
      "======================================================================\n",
      "ERROR: test_primary_key (__main__.TestCharStats.test_primary_key)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_5672/3399858392.py\", line 44, in test_primary_key\n",
      "    df = cls.spark.read.parquet(\"/home/jovyan/work/digital/target/char_stats/\")\n",
      "       ^^^\n",
      "NameError: name 'cls' is not defined\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.002s\n",
      "\n",
      "FAILED (errors=1)\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "import yaml\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "#spark = SparkSession.builder.appName(\"TestCharStats\").getOrCreate()\n",
    "\n",
    "class TestCharStats(unittest.TestCase):\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        # Loading configuration from config.yaml\n",
    "        with open(\"config.yaml\", \"r\") as config_file:\n",
    "            config = yaml.safe_load(config_file)\n",
    "\n",
    "        # Create a Spark session\n",
    "        cls.spark = SparkSession.builder.appName(\"TestCharStats\").getOrCreate()\n",
    "        \n",
    "        # Assign configuration variables\n",
    "        cls.table_name = config[\"table_name\"]\n",
    "        cls.primary_key_columns = config[\"primary_key_columns\"]\n",
    "        cls.threshold_percentage = config[\"threshold_percentage\"]\n",
    "        print(f\"printing table name:{cls.table_name}\")\n",
    "\n",
    "    @classmethod\n",
    "    def tearDownClass(cls):\n",
    "        # Stop the Spark session\n",
    "        spark.stop()\n",
    "\n",
    "    def log_test_result(self, test_name, status, reason=\"\"):\n",
    "        timestamp = datetime.datetime.now()\n",
    "        # Creating a dictionary for the test result\n",
    "        result = {\n",
    "            \"Table Name\":table_name,\n",
    "            \"Test Name\": test_name,\n",
    "            \"Status\": status,\n",
    "            \"Reason\": reason,\n",
    "            \"Timestamp\": timestamp\n",
    "        }\n",
    "        # Append the result to the test_results list\n",
    "        test_results.append(result)\n",
    "\n",
    "    def test_primary_key(self):\n",
    "        # Loading the current data\n",
    "        df = cls.spark.read.parquet(\"/home/jovyan/work/digital/target/char_stats/\")  \n",
    "\n",
    "        # Checking primary key columns to make sure no duplicates\n",
    "        num_rows = df.count()\n",
    "        num_distinct_rows = df.select(*primary_key_columns).distinct().count()\n",
    "        #self.assertEqual(num_rows, num_distinct_rows)\n",
    "        if num_rows == num_distinct_rows:\n",
    "            self.log_test_result(\"test_primary_key\", \"Pass\")\n",
    "        else:\n",
    "            self.log_test_result(\"test_primary_key\", \"Fail\", \"Duplicate primary key values found\")\n",
    "\n",
    "    def test_count_increase(self):\n",
    "        # Loading the today's and previous day data for the comparision of countyour DataFrames for previous and current loads\n",
    "        df_previous = cls.spark.read.parquet(\"/home/jovyan/work/digital/target/char_stats/\")\n",
    "        df_current = cls.spark.read.parquet(\"/home/jovyan/work/digital/target/char_stats/\")  # Replace with the actual path\n",
    "\n",
    "        # Calculating the count of increased percentage\n",
    "        count_previous = df_previous.count()\n",
    "        count_current = df_current.count()\n",
    "        increase_percentage = (count_current - count_previous) / count_previous * 100\n",
    "\n",
    "        if increase_percentage >= cls.threshold_percentage:\n",
    "            self.log_test_result(\"test_count_increase\", \"Pass\")\n",
    "        else:\n",
    "            self.log_test_result(\"test_count_increase\", \"Fail\", \"Count increase percentage is below threshold\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Creating an empty list to store test results\n",
    "    test_results = []\n",
    "\n",
    "    # Creating a test suite\n",
    "    test_suite = unittest.TestLoader().loadTestsFromTestCase(TestCharStats)\n",
    "\n",
    "    # Running the tests\n",
    "    test_runner = unittest.TextTestRunner()\n",
    "\n",
    "    # Run each test and loggig the results\n",
    "    for test_case in test_suite:\n",
    "        test_result = test_runner.run(test_case)\n",
    "\n",
    "    # Converting the test results to write into a file\n",
    "    results_df = pd.DataFrame(test_results)\n",
    "\n",
    "    # Write the results to a CSV file\n",
    "    results_df.to_csv(\"test_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1a49103f-5087-4b46-9175-873e25b87190",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 1.676s\n",
      "\n",
      "OK\n",
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 1.325s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "import yaml\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "log_file_name = f\"UnitTest1_{datetime.datetime.now().strftime('%Y-%m-%d')}.log\"\n",
    "logging.basicConfig(filename=log_file_name, level=logging.INFO)\n",
    "\n",
    "class TestCharStats(unittest.TestCase):\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        # Loading configuration from config.yaml\n",
    "        with open(\"config.yaml\", \"r\") as config_file:\n",
    "            config = yaml.safe_load(config_file)\n",
    "\n",
    "        # Create a Spark session\n",
    "        cls.spark = SparkSession.builder.appName(\"TestCharStats\").getOrCreate()\n",
    "        \n",
    "        # Assign configuration variables\n",
    "        cls.table_name = config[\"table_name\"]\n",
    "        cls.primary_key_columns = config[\"primary_key_columns\"]\n",
    "        cls.threshold_percentage = config[\"threshold_percentage\"]\n",
    "\n",
    "    @classmethod\n",
    "    def tearDownClass(cls):\n",
    "        # Stop the Spark session\n",
    "        cls.spark.stop()\n",
    "\n",
    "    def log_test_result(self, test_name, status, reason=\"\"):\n",
    "        timestamp = datetime.datetime.now()\n",
    "        # Creating a dictionary for the test result\n",
    "        result = {\n",
    "            \"Table Name\": table_name,\n",
    "            \"Test Name\": test_name,\n",
    "            \"Status\": status,\n",
    "            \"Reason\": reason,\n",
    "            \"Timestamp\": timestamp\n",
    "        }\n",
    "        # Append the result to the test_results list\n",
    "        test_results.append(result)\n",
    "\n",
    "        # Logging the result\n",
    "        log_message = f\"Test Name: {test_name}, Status: {status}, Reason: {reason}\"\n",
    "        logging.info(log_message)\n",
    "\n",
    "    def test_primary_key(self):\n",
    "        # Loading the current data\n",
    "        df = spark.read.parquet(\"/home/jovyan/work/digital/target/char_stats/\")  \n",
    "\n",
    "        # Checking primary key columns to make sure no duplicates\n",
    "        num_rows = df.count()\n",
    "        num_distinct_rows = df.select(*primary_key_columns).distinct().count()\n",
    "\n",
    "        if num_rows == num_distinct_rows:\n",
    "            self.log_test_result(\"test_primary_key\", \"Pass\")\n",
    "        else:\n",
    "            self.log_test_result(\"test_primary_key\", \"Fail\", \"Duplicate primary key values found\")\n",
    "\n",
    "    def test_count_increase(self):\n",
    "        # Loading the today's and previous day data for the comparison of count\n",
    "        df_previous = spark.read.parquet(\"/home/jovyan/work/digital/target/char_stats/\")\n",
    "        df_current = spark.read.parquet(\"/home/jovyan/work/digital/target/char_stats/\")  # Replace with the actual path\n",
    "\n",
    "        # Calculating the count of increased percentage\n",
    "        count_previous = df_previous.count()\n",
    "        count_current = df_current.count()\n",
    "        increase_percentage = (count_current - count_previous) / count_previous * 100\n",
    "\n",
    "        if increase_percentage >= threshold_percentage:\n",
    "            self.log_test_result(\"test_count_increase\", \"Pass\")\n",
    "        else:\n",
    "            self.log_test_result(\"test_count_increase\", \"Fail\", \"Count increase percentage is below threshold\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Creating an empty list to store test results\n",
    "    test_results = []\n",
    "\n",
    "    # Creating a test suite\n",
    "    test_suite = unittest.TestLoader().loadTestsFromTestCase(TestCharStats)\n",
    "\n",
    "    # Running the tests\n",
    "    test_runner = unittest.TextTestRunner()\n",
    "\n",
    "    # Run each test and logging the results\n",
    "    for test_case in test_suite:\n",
    "        test_result = test_runner.run(test_case)\n",
    "\n",
    "    # Converting the test results to write into a file\n",
    "    results_df = pd.DataFrame(test_results)\n",
    "    results_df.to_csv(\"test_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ac579e0a-abec-452a-873e-209a4c62c4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 1.465s\n",
      "\n",
      "OK\n",
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 1.123s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "import yaml\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "log_file_name = f\"UnitTest_{datetime.datetime.now().strftime('%Y-%m-%d')}.log\"\n",
    "logging.basicConfig(filename=log_file_name, level=logging.INFO)\n",
    "\n",
    "class TestCharStats(unittest.TestCase):\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        # Loading configuration from config.yaml\n",
    "        with open(\"config.yaml\", \"r\") as config_file:\n",
    "            config = yaml.safe_load(config_file)\n",
    "\n",
    "        # Create a Spark session\n",
    "        cls.spark = SparkSession.builder.appName(\"TestCharStats\").getOrCreate()\n",
    "        \n",
    "        # Assign configuration variables\n",
    "        cls.table_name = config[\"table_name\"]\n",
    "        cls.primary_key_columns = config[\"primary_key_columns\"]\n",
    "        cls.threshold_percentage = config[\"threshold_percentage\"]\n",
    "\n",
    "    @classmethod\n",
    "    def tearDownClass(cls):\n",
    "        # Stop the Spark session\n",
    "        cls.spark.stop()\n",
    "\n",
    "    def log_message(self, message):\n",
    "        # Log messages, including comments\n",
    "        logging.info(message)\n",
    "\n",
    "    def log_test_result(self, test_name, status, reason=\"\"):\n",
    "        timestamp = datetime.datetime.now()\n",
    "        # Creating a dictionary for the test result\n",
    "        result = {\n",
    "            \"Table Name\": table_name,\n",
    "            \"Test Name\": test_name,\n",
    "            \"Status\": status,\n",
    "            \"Reason\": reason,\n",
    "            \"Timestamp\": timestamp\n",
    "        }\n",
    "        # Append the result to the test_results list\n",
    "        test_results.append(result)\n",
    "\n",
    "        # Logging the test result\n",
    "        log_message = f\"Test Name: {test_name}, Status: {status}, Reason: {reason}\"\n",
    "        logging.info(log_message)\n",
    "\n",
    "    def test_primary_key(self):\n",
    "        # Log comment\n",
    "        self.log_message(\"Loading the current data\")\n",
    "\n",
    "        # Loading the current data\n",
    "        df = spark.read.parquet(\"/home/jovyan/work/digital/target/char_stats/\")  \n",
    "\n",
    "        # Checking primary key columns to make sure no duplicates\n",
    "        num_rows = df.count()\n",
    "        num_distinct_rows = df.select(*primary_key_columns).distinct().count()\n",
    "\n",
    "        if num_rows == num_distinct_rows:\n",
    "            self.log_test_result(\"test_primary_key\", \"Pass\")\n",
    "        else:\n",
    "            self.log_test_result(\"test_primary_key\", \"Fail\", \"Duplicate primary key values found\")\n",
    "\n",
    "    def test_count_increase(self):\n",
    "        # Log comment\n",
    "        self.log_message(\"Loading the today's and previous day data for the comparison of count\")\n",
    "\n",
    "        # Loading the today's and previous day data for the comparison of count\n",
    "        df_previous = spark.read.parquet(\"/home/jovyan/work/digital/target/char_stats/\")\n",
    "        df_current = spark.read.parquet(\"/home/jovyan/work/digital/target/char_stats/\")  # Replace with the actual path\n",
    "\n",
    "        # Calculating the count of increased percentage\n",
    "        count_previous = df_previous.count()\n",
    "        count_current = df_current.count()\n",
    "        increase_percentage = (count_current - count_previous) / count_previous * 100\n",
    "\n",
    "        if increase_percentage >= threshold_percentage:\n",
    "            self.log_test_result(\"test_count_increase\", \"Pass\")\n",
    "        else:\n",
    "            self.log_test_result(\"test_count_increase\", \"Fail\", \"Count increase percentage is below threshold\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Creating an empty list to store test results\n",
    "    test_results = []\n",
    "\n",
    "    # Creating a test suite\n",
    "    test_suite = unittest.TestLoader().loadTestsFromTestCase(TestCharStats)\n",
    "\n",
    "    # Running the tests\n",
    "    test_runner = unittest.TextTestRunner()\n",
    "\n",
    "    # Run each test and logging the results\n",
    "    for test_case in test_suite:\n",
    "        test_result = test_runner.run(test_case)\n",
    "\n",
    "    # Converting the test results to write into a file\n",
    "    results_df = pd.DataFrame(test_results)\n",
    "    results_df.to_csv(\"test_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c7472fd6-cc6a-4e29-a3ae-bfe332a3243c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python UnitTesting.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1643855e-8a8b-4466-906d-802093df4511",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
